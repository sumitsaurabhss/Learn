{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c3590ee-a3c9-4e27-a163-9cff9a5a5352",
   "metadata": {},
   "source": [
    "# Language Model (LM)\n",
    "Language Model gives the probability distribution over a sequence of tokens.\n",
    "\n",
    "![LMs can 'Generate' Text](Images/LM.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1e1056-81a8-45eb-bc67-f3aec4878486",
   "metadata": {},
   "source": [
    "# 'Large' Language Models\n",
    "The 'Large' in terms of model's size (number of parameters) and massive size of training dataset.\n",
    "\n",
    "![LLMs in AI Landscape](Images/LLM.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2efd87-40bc-4524-a5c8-3b6d4b6af58a",
   "metadata": {},
   "source": [
    "## LLMs Evolution\n",
    "\n",
    "[Evolution of LLMs](https://synthedia.substack.com/p/a-timeline-of-large-language-model)\n",
    "\n",
    "- Google introduced Transformer architecture in 2017 in \"Attention Is All You Need\" paper. It also released BERT model in 2018 for language translation which achieved SOTA (State Of The Art) on 11 NLP tasks. But due to its large parameters size at that time, small models like DistilBERT, TinyBERT, MobileBERT based on BERT were proposed. BERT is an Encoder-only model. This started the beginning of use of Transformer as Language Representation Models.\n",
    "\n",
    "- OpenAI published \"Improving Language Understanding by Generative Pre-Training\" in 2018 and released GPT-1(117M, 512 tokens) model which used Decoder-only architecture. It also introduced the idea of generative pre-training over large corpus.\n",
    "\n",
    "- OpenAI published \"Language Models are Unsupervised Multitask Learners\" paper in 2019 and also released GPT-2(1.5B, 1024 tokens) model.\n",
    "\n",
    "- Google developed T5 (Text-To-Text Transfer Transformer) model in 2019. It used Encoder-Decoder architecture.\n",
    "\n",
    "- Meta released RoBERTa model and published \"RoBERTa: A Robustly Optimized Pretraining Approach\" paper in 2019. It found that BERT was significantly undertrained.\n",
    "\n",
    "- Meta also released XLM model along with \"Cross-lingual Language Model Pretraining\" paper. It proposed methods to learn cross-lingual language models (XLMs). It obtained SOTA on cross-lingual classification, and unsupervised and supervised machine translation.\n",
    "\n",
    "- OpenAI released GPT-3(175B) model and \"Language Models are Few-Shot Learners\" in 2020. It observed the phenomana of In-context learning. OpenAI stopped open-sourcing.\n",
    "\n",
    "- Google released PalM model and \"PaLM: Scaling Language Modeling with Pathways\" paper in 2022. It stopped open-sourcing.\n",
    "\n",
    "- Meta released OPT model and \"OPT: Open Pre-trained Transformer Language Models\" in 2022. It is a suite of Decoder-only pre-trained transformers ranging from 125M to 175B parameters. It promotes open-sourcing.\n",
    "\n",
    "![Models in 2023](Images/2023Models.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63af8ae4-2fd2-41ac-bb6b-eb7bfb10ec48",
   "metadata": {},
   "source": [
    "LLMs show emergent capabilities, not observed previously in ‘small’ LMs.\n",
    "- In-context learning: A pre-trained language model can be guided with only prompts to perform different tasks (without separate task-specific fine-tuning).\n",
    "- In-context learning is an example of emergent behavior.\n",
    "\n",
    "LLMs are widely adopted in real-world.\n",
    "- Research: LLMs have transformed NLP research world, achieving state-of-the-art performance across a wide range of tasks such as sentiment classification, question answering, summarization, and machine translation.\n",
    "- Industry: Here is a very incomplete list of some high profile large language models that are being used in production systems:\n",
    "  - [Google Search](https://blog.google/products/search/search-language-understanding-bert/) (BERT)\n",
    "  -  [Facebook content moderation](https://ai.meta.com/blog/harmful-content-can-evolve-quickly-our-new-ai-system-adapts-to-tackle-it/) (XLM)\n",
    "  - [Microsoft’s Azure OpenAI Service](https://news.microsoft.com/source/features/innovation/new-azure-openai-service/) (GPT-3/3.5/4)\n",
    "\n",
    "With tremendous capabilities, LLMs’ usage also carries various risks.\n",
    "- Reliability & Disinformation: LLMs often hallucinate – generate responses that seem correct, but are not factually correct.\n",
    "  - Significant challenge for high-stakes applications like healthcare\n",
    "- Social bias: Most LLMs show performance disparities across demographic groups, and their predictions can enforce stereotypes.\n",
    "  - P(He is a doctor) > P(She is a doctor.)\n",
    "  - Training data contains inherent bias\n",
    "- Toxicity: LLMs can generate toxic/hateful content.\n",
    "  - Trained on a huge amount of Internet data (e.g., Reddit), which inevitably contains offensive content\n",
    "  - Challenge for applications such as writing assistants or chatbots\n",
    "- Security: LLMs are trained on a scrape of the public Internet - anyone can put up a website that can enter the training data.\n",
    "  - An attacker can perform a data poisoning attack.\n",
    "Credits: https://stanford-cs324.github.io/winter2022/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3aa0f3-8613-408c-859e-7e356c491086",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
