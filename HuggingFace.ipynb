{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e5e6640-ee28-4a72-bd1e-50adce698264",
   "metadata": {},
   "source": [
    "# Hugging Face\n",
    "The company is recognized for its open-source library, Transformers, which has democratized access to state-of-the-art NLP models and tools for both researchers and developers. It offers everything from tokenizers, which help computers make sense of text, to a huge variety of ready-to-go language models, and even a treasure trove of data suited for language tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89864244-e2ec-4432-972c-5864f8c3b06b",
   "metadata": {},
   "source": [
    "## Key Components\n",
    "- Tokenizers: These are essential for converting text into a format that models can understand, acting like a translator for words. These work like a translator, converting the words we use into smaller parts and creating a secret code that computers can understand and work with.\n",
    "- Models: Hugging Face offers a vast library of pre-trained models, including popular ones like BERT and GPT, which serve as the brains for NLP tasks. These are like the brain for computers, allowing them to learn and make decisions based on information they've been fed.\n",
    "- Datasets: The datasets library provides a standardized way to access and utilize various datasets tailored for NLP tasks, akin to textbooks for machine learning models. They are collections of information that models study to learn and improve.\n",
    "- Trainers: Trainers simplify the model training process by providing a unified interface to train models efficiently, implementing the PyTorch training loop. They are the coaches for computer models. They help these models get better at their tasks by practicing and providing guidance. HuggingFace Trainers implement the PyTorch training loop for you, so you can focus instead on other aspects of working on the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c403b49-0524-4ed8-9f3d-d4c92e8f28ac",
   "metadata": {},
   "source": [
    "## Hugging Face Tokenizers\n",
    "HuggingFace tokenizers help us break down text into smaller, manageable pieces called tokens. These tokenizers are easy to use and also remarkably fast due to their use of the Rust programming language.\n",
    "\n",
    "Tokenization: It is a critical pre-processing step in NLP that involves breaking down text into smaller units called tokens, which can be words, characters, or subwords, to make it easier to analyze.\n",
    "\n",
    "Tokens: These are the pieces you get after cutting up text during tokenization, kind of like individual Lego blocks that can be words, parts of words, or even single letters. These tokens are converted to numerical values for models to understand.\n",
    "\n",
    "Pre-trained Model: This is a ready-made model that has been previously taught with a lot of data.\n",
    "\n",
    "Uncased: This means that the model treats uppercase and lowercase letters as the same.\n",
    "\n",
    "Numerical Representation: The tokens are converted into numerical values that the model uses internally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37a56ecd-45ab-4684-82c9-66816f90e1e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install -qU --user transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7dc8479b-bb07-47e9-96b9-2ad23925c720",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'transformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BertTokenizer\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Initialize the tokenizer\u001b[39;00m\n\u001b[0;32m      4\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m BertTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbert-base-uncased\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'transformers'"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# See how many tokens are in the vocabulary\n",
    "tokenizer.vocab_size\n",
    "# 30522"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a009d648-1c79-42aa-9b75-0732e28ab709",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the sentence\n",
    "tokens = tokenizer.tokenize(\"I heart Generative AI\")\n",
    "\n",
    "# Print the tokens\n",
    "print(tokens)\n",
    "# ['i', 'heart', 'genera', '##tive', 'ai']\n",
    "\n",
    "# Show the token ids assigned to each token\n",
    "print(tokenizer.convert_tokens_to_ids(tokens))\n",
    "# [1045, 2540, 11416, 6024, 9932]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fecdd9d-e5af-4309-bab5-d00f861e66b7",
   "metadata": {},
   "source": [
    "## Hugging Face Models\n",
    "Hugging Face models provide a quick way to get started using models trained by the community. The library includes state-of-the-art models such as BERT, GPT-2, and RoBERTa, which have set benchmarks in different NLP domains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd83f0f-e890-444c-ba9e-a1fc2a53a12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "\n",
    "# Load a pre-trained sentiment analysis model\n",
    "model_name = \"textattack/bert-base-uncased-imdb\"\n",
    "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "# Tokenize the input sequence\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "inputs = tokenizer(\"I love Generative AI\", return_tensors=\"pt\")\n",
    "\n",
    "# Make prediction\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs).logits\n",
    "    probabilities = torch.nn.functional.softmax(outputs, dim=1)\n",
    "    predicted_class = torch.argmax(probabilities)\n",
    "\n",
    "# Display sentiment result\n",
    "if predicted_class == 1:\n",
    "    print(f\"Sentiment: Positive ({probabilities[0][1] * 100:.2f}%)\")\n",
    "else:\n",
    "    print(f\"Sentiment: Negative ({probabilities[0][0] * 100:.2f}%)\")\n",
    "# Sentiment: Positive (88.68%)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d27c743-f553-4038-94cf-dce47b97ac67",
   "metadata": {},
   "source": [
    "## Hugging Face Datasets\n",
    "HuggingFace Datasets library is a powerful tool for managing a variety of data types, like text and images, efficiently and easily. This resource is incredibly fast and doesn't use a lot of computer memory, making it great for handling big projects without any hassle. It is designed to simplify the process of accessing, pre-processing, and managing large datasets for machine learning projects.\n",
    "\n",
    "Purpose of the Library: The library provides a unified API that allows users to access a wide variety of datasets, including text, audio, and image datasets, making it easier to work with data in machine learning.\n",
    "\n",
    "Efficiency: Built on Apache Arrow, the library is optimized for speed and memory efficiency, allowing for fast data processing even with large datasets.\n",
    "\n",
    "Tokenization: The library supports tokenization, which is essential for preparing text data for model input. The video highlights how the dataset can be easily manipulated to extract and display relevant information.\n",
    "\n",
    "Reproducibility and Versioning: The library promotes reproducibility in research by allowing users to access specific versions of datasets and even contribute their own datasets.\n",
    "\n",
    "IMDb dataset: A dataset of movie reviews that can be used to train a machine learning model to understand human sentiments.\n",
    "\n",
    "Apache Arrow: A software framework that allows for fast data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95823f51-bc8c-44c0-ba2a-96a4a82779ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "# Load the IMDB dataset, which contains movie reviews\n",
    "# and sentiment labels (positive or negative)\n",
    "dataset = load_dataset(\"imdb\")\n",
    "\n",
    "# Fetch a revie from the training set\n",
    "review_number = 42\n",
    "sample_review = dataset[\"train\"][review_number]\n",
    "\n",
    "display(HTML(sample_review[\"text\"][:450] + \"...\"))\n",
    "# WARNING: This review contains SPOILERS. Do not read if you don't want some points revealed to you before you watch the\n",
    "# film.\n",
    "# \n",
    "# With a cast like this, you wonder whether or not the actors and actresses knew exactly what they were getting into. Did they\n",
    "# see the script and say, `Hey, Close Encounters of the Third Kind was such a hit that this one can't fail.' Unfortunately, it does.\n",
    "# Did they even think to check on the director's credentials...\n",
    "\n",
    "if sample_review[\"label\"] == 1:\n",
    "    print(\"Sentiment: Positive\")\n",
    "else:\n",
    "    print(\"Sentiment: Negative\")\n",
    "# Sentiment: Negative"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a1dc690-5be7-463b-817c-838875420c7f",
   "metadata": {},
   "source": [
    "## Hugging Face Trainers\n",
    "Hugging Face Trainer class simplifies the process of training and fine-tuning machine learning models. It offer a simplified approach to training generative AI models, making it easier to set up and run complex machine learning tasks. This tool wraps up the hard parts, like handling data and carrying out the training process, allowing us to focus on the big picture and achieve better outcomes with our AI endeavors.\n",
    "\n",
    "Purpose of the Trainer: The Trainer class encapsulates the complexities of training loops, evaluation, and optimization, allowing users to focus on model performance rather than implementation details.\n",
    "\n",
    "Tokenization Process: A function called tokenize_function is created to convert text reviews into tokens that the model can understand. It includes truncating long strings and padding shorter ones to ensure uniform input length.\n",
    "\n",
    "Truncating: This refers to shortening longer pieces of text to fit a certain size limit.\n",
    "\n",
    "Padding: Adding extra data to shorter texts to reach a uniform length for processing.\n",
    "\n",
    "Batches: Batches are small, evenly divided parts of data that the AI looks at and learns from each step of the way.\n",
    "\n",
    "Batch Size: The number of data samples that the machine considers in one go during training.\n",
    "\n",
    "Epochs: A complete pass through the entire training dataset. The more epochs, the more the computer goes over the material to learn.\n",
    "\n",
    "Dataset Splits: Dividing the dataset into parts for different uses, such as training the model and testing how well it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a463ee00-f8ac-4141-84a2-8ded8bb162a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (DistilBertForSequenceClassification,\n",
    "    DistilBertTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "from datasets import load_dataset\n",
    "\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\", num_labels=2\n",
    ")\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "\n",
    "dataset = load_dataset(\"imdb\")\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size=64,\n",
    "    output_dir=\"./results\",\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=3,\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c035df-3f06-4d5c-8e6e-a3cacd57b37c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (DS)",
   "language": "python",
   "name": ".env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
