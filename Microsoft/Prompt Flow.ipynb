{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5619394c",
   "metadata": {},
   "source": [
    "# Prompt Flow\n",
    "Prompt flow allows you to create flows, which refers to the sequence of actions or steps that are taken to achieve a specific task or functionality. A flow represents the overall process or pipeline that incorporates the interaction with the LLM to address a particular use case. The flow encapsulates the entire journey from receiving input to generating output or performing a desired action."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f8aebc",
   "metadata": {},
   "source": [
    "## Development lifecycle of a large language model (LLM) app\n",
    "1. **Initialization**: Define the use case and design the solution.\n",
    "2. **Experimentation**: Develop a flow and test with a small dataset.\n",
    "3. **Evaluation and refinement**: Assess the flow with a larger dataset.\n",
    "4. **Production**: Deploy and monitor the flow and application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c14ce7fe",
   "metadata": {},
   "source": [
    "### Initialization\n",
    "1. Define the **objective**\n",
    "2. Collect a **sample dataset**\n",
    "3. Build a **basic prompt**\n",
    "4. Design the **flow**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9247eb56",
   "metadata": {},
   "source": [
    "### Experimentation\n",
    "The *experimentation* phase is an iterative process during which you  \n",
    "(1) **run** the flow against a sample dataset. You then  \n",
    "(2) **evaluate** the prompt's performance. If you're  \n",
    "(3) satisfied with the result, you can **move on** to evaluation and refinement. If there's  \n",
    "(4) room for improvement, you can **modify** the flow by changing the prompt or flow itself."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b970f1c2",
   "metadata": {},
   "source": [
    "### Evaluation and refinement\n",
    "By testing the flow on a larger dataset, you can evaluate how well the LLM application generalizes to new data. During evaluation, you can identify potential bottlenecks or areas for optimization or refinement.\n",
    "\n",
    "When you edit your flow, you should first run it against a smaller dataset before running it again against a larger dataset. Testing your flow with a smaller dataset allows you to more quickly respond to any issues.\n",
    "\n",
    "Once your LLM application appears to be robust and reliable in handling various scenarios, you can decide to move the LLM application to production."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf28ba3e",
   "metadata": {},
   "source": [
    "### Production\n",
    "1. **Optimize** the flow for efficiency and effectiveness.\n",
    "2. **Deploy** your flow to an endpoint. When you call the endpoint, the flow is triggered to run and the desired output is generated.\n",
    "3. **Monitor** the performance of your solution by collecting usage data and end-user feedback. By understanding how the application performs, you can improve the flow whenever necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b797265c",
   "metadata": {},
   "source": [
    "## Core Components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9365239",
   "metadata": {},
   "source": [
    "### Flow\n",
    "Flows are executable workflows often consist of three parts:\n",
    "- **Inputs**: Represent data passed into the flow. Can be different data types like strings, integers, or boolean.\n",
    "- **Nodes**: Represent tools that perform data processing, task execution, or algorithmic operations.\n",
    "- **Outputs**: Represent the data produced by the flow.\n",
    "\n",
    "Similar to a pipeline, a flow can consist of multiple nodes that can use the flow's inputs or any output generated by another node. You can add a node to a flow by choosing one of the available types of **tools**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6093ad6c",
   "metadata": {},
   "source": [
    "### Tool\n",
    "- **LLM tool**: Enables custom prompt creation utilizing Large Language Models.\n",
    "- **Python tool**: Allows the execution of custom Python scripts.\n",
    "- **Prompt tool**: Prepares prompts as strings for complex scenarios or integration with other tools.\n",
    "\n",
    "Each tool is an executable unit with a specific function. You can use a tool to perform tasks like summarizing text, or making an API call. You can use multiple tools within one flow and use a tool multiple times.\n",
    "\n",
    "Whenever you add a new node to your flow, adding a new tool, you can define the expected inputs and outputs. A node can use one of the whole flow's inputs, or another node's output, effectively linking nodes together."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38aaa02",
   "metadata": {},
   "source": [
    "## Types of flows\n",
    "- **Standard flow**: Ideal for general LLM-based application development, offering a range of versatile tools.\n",
    "- **Chat flow**: Designed for conversational applications, with enhanced support for chat-related functionalities.\n",
    "- **Evaluation flow**: Focused on performance evaluation, allowing the analysis and improvement of models or applications through feedback on previous runs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adba1622",
   "metadata": {},
   "source": [
    "## Connections\n",
    "Whenever you want your flow to connect to external data source, service, or API, you need your flow to be authorized to communicate with that external service. When you create a **connection**, you configure a secure link between prompt flow and external services, ensuring seamless and safe data communication.\n",
    "\n",
    "Connection securely stores the endpoint, API key, or credentials necessary for prompt flow to communicate with the external service. Any necessary secrets aren't exposed to users, but instead are stored in an Azure Key Vault."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2601d3dd",
   "metadata": {},
   "source": [
    "## Runtimes\n",
    "After creating your flow, and configuring the necessary connections your tools use, you want to run your flow. To run the flow, you need compute, which is offered through prompt flow **runtimes**.\n",
    "\n",
    "Runtimes are a combination of  \n",
    "(1) a **compute instance** providing the necessary compute resources, and  \n",
    "(2) **an environment** specifying the necessary packages and libraries that need to be installed before being able to run the flow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdac20b2",
   "metadata": {},
   "source": [
    "## Variants and monitoring options\n",
    "You can optimize your flow by using **variants**, you can deploy your flow to an **endpoint**, and you can monitor your flow by evaluating key metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c8af46",
   "metadata": {},
   "source": [
    "### Variants\n",
    "Prompt flow **variants** are versions of a tool node with distinct settings. Currently, variants are only supported in the LLM tool, where a variant can represent a different prompt content or connection setting.\n",
    "\n",
    "Some benefits of using variants are:\n",
    "- **Enhance the quality of your LLM generation**: Creating diverse variants of an LLM node helps find the best prompt and settings for high-quality content.\n",
    "- **Save time and effort**: Variants allow for easy management and comparison of different prompt versions, streamlining historical tracking and reducing the effort in prompt tuning.\n",
    "- **Boost productivity**: They simplify the optimization of LLM nodes, enabling quicker creation and management of variations, leading to better results in less time.\n",
    "- **Facilitate easy comparison**: Variants enable side-by-side result comparisons, aiding in choosing the most effective variant based on data-driven decisions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a3ada6",
   "metadata": {},
   "source": [
    "### Endpoint\n",
    "When you're satisfied with the performance of your flow, you can choose to deploy it to an **online endpoint**. Endpoints are URLs that you can call from any application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737d0075",
   "metadata": {},
   "source": [
    "### Monitor\n",
    "In prompt flow, monitoring evaluation metrics is key to understanding your LLM application's performance, ensuring they meet real-world expectations and deliver accurate results.\n",
    "\n",
    "#### Metrics\n",
    "The key metrics used for monitoring evaluation in prompt flow each offer unique insight into the performance of LLMs:\n",
    "- **Groundedness**: Measures alignment of the LLM application's output with the input source or database.\n",
    "- **Relevance**: Assesses how pertinent the LLM application's output is to the given input.\n",
    "- **Coherence**: Evaluates the logical flow and readability of the LLM application's text.\n",
    "- **Fluency**: Assesses the grammatical and linguistic accuracy of the LLM application's output.\n",
    "- **Similarity**: Quantifies the contextual and semantic match between the LLM application's output and the ground truth."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628ef852",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
