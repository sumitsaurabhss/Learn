{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d36b838-a6f2-4a94-8ca8-e31eec5ee7ee",
   "metadata": {},
   "source": [
    "# Transformer Model Architecture\n",
    "The transformer model is a deep-learning architecture that has revolutionized natural language processing (NLP) and sequence-oriented tasks with the help of a self-attention mechanism. Introduced by Google’s researchers in the paper \"Attention is All You Need,\" it is designed to process sequential data to transform an input to an appropriate output.  \n",
    "The transformer model is a powerful architecture for predicting answers in natural language processing. It efficiently processes text, capturing word relationships through attention mechanisms. Its multi-head attention allows the simultaneous processing of different parts of the input."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4076e6-d803-412e-96d9-0a3335021559",
   "metadata": {},
   "source": [
    "## Self-attention mechanism\n",
    "The self-attention mechanism allows the transformer model to determine the importance of different positions in the input sequence for making predictions. Self-attention helps the model identify relevant parts of the sequence by comparing positions using query, key, and value matrices. By assigning weights to each position, self-attention calculates a weighted sum of the values and creates a context vector representing the sequence's most important parts. Self-attention enables transformer models to consider context and generate more accurate and nuanced outputs than traditional neural networks.\n",
    "\n",
    "The transformer model architecture is depicted in the following illustration:  \n",
    "<img src=\"../Images/transformer-1.png\" alt=\"The architecture of Transformer model\" width=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8beae0fe-8f59-403a-bec1-be65ff67e193",
   "metadata": {},
   "source": [
    "## Embeddings\n",
    "Embeddings convert input or output tokens to numerical representations that the transformer model can process. This is a way to represent words as dense vectors of real numbers. This captures the contextual meanings of words and their relationships with other words in the sentence. The transformer model uses positional embeddings to find the order of each word in the input or output sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b6e0e9-e862-44f9-beed-6ec9f836868e",
   "metadata": {},
   "source": [
    "## Positional encoding\n",
    "Positional encoding is utilized in the transformer model to incorporate information about the word order. The model itself lacks this built-in understanding. It involves adding a fixed vector representation to each word's embedding. By adding positional encoding to the embeddings, the model can distinguish between words based on their position within the sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70409db0-d3e5-4be9-bfb4-c84ba94557ab",
   "metadata": {},
   "source": [
    "## Multi-head attention\n",
    "The multi-head attention allows the model to simultaneously focus on the distinct parts of the input sequence by splitting it into subsets to produce more robust results. It involves calculating weighted sums using queries, keys, and values and employing softmax functions. The multi-head attention mechanism enables the model to capture complex relationships between different parts of the input sequence, improving accuracy and effectiveness."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01eb1cdb-3519-46df-898e-baf830685b28",
   "metadata": {},
   "source": [
    "## Feed-forward network\n",
    "The position-wise feed-forward network improves the ability of the transformer model to predict answers by processing each position separately and in parallel to capture the dependencies between different positions effectively. It uses the rectified linear activation function, which multiplies the input vector with the weight matrix to calculate the output for each position in the input sequence. The feed-forward network has two advantages: it allows the model to learn nonlinear relationships between positions and reduces computational complexity due to parallel processing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92954e5d-4147-409f-9f56-80651c385bde",
   "metadata": {},
   "source": [
    "## Softmax function\n",
    "The softmax function is applied to the text generated from the self-attention layer to normalize scores and assign probabilities to each output token. The output token with the highest probability is then selected to predict the answer. The transformer model uses beam search to reduce the softmax's complexity in case of a large output vocabulary. The formula for the softmax function is shown below:\n",
    "$$\\sigma(\\vec{x_{i}})=\\frac{e^{x_{i}}}{\\sum_{j=1}^{k} e^{x_{j}}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df38f354-1349-4c0d-a088-1b20fbe68d0a",
   "metadata": {},
   "source": [
    "## Pretraining LLMs under the hood\n",
    "Transformers have revolutionized the field of NLP. Most of the state-of-the-art language models today, such as GPT, Llama, BERT, etc., are built on the transformer architecture to understand and generate human-like text. Transformers can consist of an encoder-decoder architecture, but many models use only one of these components. The encoder takes an input and generates its representation. The representation is passed to the decoder, which generates an output. This architecture helps the models to learn complex details and patterns of data during pretraining.  \n",
    "<img src=\"../Images/transformer-2.png\" alt=\"The architecture of Transformer model\" width=\"750\">\n",
    "\n",
    "While pretraining, the model is actually trained on a large corpus of data. The pretraining process involves the following layers:\n",
    "\n",
    "- Input embedding layer: It converts the input into the numerical representation called embeddings.\n",
    "\n",
    "- Positional encoding layer: It adds information about the position of the word to the input embedding and forwards this combined information to the encoder.\n",
    "\n",
    "- Encoder layers: The encoder, consisting of multiple sublayers, uses the self-attention mechanisms to help the model understand the context and relationship between words of the input. The multi-head attention layer of the encoder computes the attention matrix for the input embedding and passes it to the feedforward layer, which generates the representation of the input. The add & norm component is applied after each sublayer of the encoder, which combines the input of the layer with the output (residual connection) and normalizes the activations to stabilize the process.\n",
    "\n",
    "- Decoder layers: The decoder is also made of multiple sublayers that generate the output sequence. First, the masked multi-head attention layer computes the attention matrix for the output embedding and passes it to the multi-head attention layer. The multi-head attention layer combines it with the encoder’s representation and generates the representation of the output. The add & norm component is applied after each sublayer of the decoder, which combines the input of the layer with the output (residual connection) and normalizes the activations to stabilize the process.\n",
    "\n",
    "- Linear layer: The linear layer converts the decoder's output to the logits of the size of the vocabulary.\n",
    "\n",
    "- Softmax layer: The softmax layer applies the softmax function to convert the logits into probabilities. A token with maximum probability is then selected as the final output.\n",
    "\n",
    "During pre-training, all these layers are trained together. The model understands the general patterns and structures in the early layers and then moves to learn specific data features in the later layer. This base concept is then used for fine-tuning, where we train the custom model for specific tasks.\n",
    "\n",
    "Attention matrix: An attention matrix is a matrix that determines the importance of each word in the input sequence with respect to all other words.  \n",
    "Logits: In the context of transformers, logits are the unnormalized output values produced by the final layer of the model.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d537f483-45ba-4df9-9e3b-0f25bee07270",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "604eb003-a98d-404d-8393-d684afa869bc",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e9ffdb15-546f-4f0c-ae4f-9b975148349c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "096863b7-5d99-4460-a362-a0889feeadae",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a4981001-2e8a-47e2-8a0c-745076cb6b86",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ae713f6d-e6a3-4ee5-8473-2bf5e0015cd6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1909d39d-f947-47e8-bac0-ec7ffa1a47ee",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
