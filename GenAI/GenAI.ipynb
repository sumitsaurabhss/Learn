{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e73095b7-e3aa-4674-a0c3-75925efbec5e",
   "metadata": {},
   "source": [
    "## Foundation Model\n",
    "A foundation model is a type of AI model that is trained on a vast amount of data at scale, allowing it to perform a wide range of tasks with minimal additional training. These models serve as a base for various applications. They excel at learning from extensive datasets, enabling them to generalize and perform well on tasks they weren't explicitly trained for.\n",
    "- Trained on vast datasets.\n",
    "- Designed to generalize across many tasks, allowing them to perform well on examples they have never seen before.\n",
    "- Require significant computational resources due to their size and complexity.\n",
    "- Examples include models like GPT from OpenAI, Bard from Google, and DALL-E.\n",
    "\n",
    "Generalize: The ability of a model to apply what it has learned from its training data to new, unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708fa68f-4273-4afd-aed6-5eaae3f65279",
   "metadata": {},
   "source": [
    "## Traditional Models:\n",
    "- Typically trained on smaller, task-specific datasets.\n",
    "- Developed from scratch based on meticulously curated data, making them task-specific and domain-specific.\n",
    "- Generally smaller in size and require less computational power.\n",
    "- Examples include linear regression, decision trees, and convolutional neural networks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23cb6d14-3818-4424-8479-0ac629eda751",
   "metadata": {},
   "source": [
    "### Key Differences:\n",
    "\n",
    "Foundation models are versatile and can adapt to various tasks, while traditional models are specialized and limited to specific applications.\n",
    "The emergence of foundation models represents a significant shift in AI, focusing on large-scale training and broad applicability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30bbe2a-60bf-4dbf-8867-0097b5f0c717",
   "metadata": {},
   "source": [
    "## Transformer Architecture\n",
    "The transformer architecture has revolutionized how machines handle sequential data, allowing for the training of large models efficiently."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd84c05-dfc5-42bb-bed5-7670fe4e3957",
   "metadata": {},
   "source": [
    "### Self-attention mechanism\n",
    "The self-attention mechanism in a transformer is a process where each element in a sequence computes its representation by attending to and weighing the importance of all elements in the sequence, allowing the model to capture complex relationships and dependencies. This is particularly useful in tasks like language modeling.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c561f7-a828-49f9-82aa-df7cbaf14c9f",
   "metadata": {},
   "source": [
    "## Benchmark\n",
    "\n",
    "Why they matter?\n",
    "Benchmarks matter because they are the standards that help us measure and accelerate progress in AI. They offer a common ground for comparing different AI models and encouraging innovation, providing important stepping stones on the path to more advanced AI technologies.\n",
    "\n",
    "Importance of Benchmark Datasets: Benchmark datasets serve as standardized testbeds for algorithms, providing clear, objective, and quantifiable metrics for evaluation.\n",
    "\n",
    "Benefits of Benchmark Datasets:\n",
    "Comparability: They allow for direct comparison of different algorithms and models.  \n",
    "Reproducibility: They create a shared foundation for reproducing and verifying results, crucial for scientific progress.  \n",
    "Focus: They concentrate research efforts on specific problems, leading to innovation.  \n",
    "Democratization: Open access to high-quality datasets levels the playing field for researchers worldwide.  \n",
    "Acceleration of Progress: As models surpass benchmarks, datasets evolve to present new challenges, driving further advancements in AI.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c38c2dc-03ee-4420-a6af-56b1bf5e512c",
   "metadata": {},
   "source": [
    "### GLUE - General Language Understanding Evaluation\n",
    "The GLUE benchmark is designed to assess the capabilities of AI models across a variety of linguistic tasks, serving as a litmus test for their understanding of human language. They serve as an essential tool to assess an AI's grasp of human language, covering diverse tasks, from grammar checking to complex sentence relationship analysis. By putting AI models through these varied linguistic challenges, we can gauge their readiness for real-world tasks and uncover any potential weaknesses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee08a8ba-8295-4595-8508-4e189b5ab288",
   "metadata": {},
   "source": [
    "#### The GLUE Tasks / Benchmarks\n",
    "##### CoLA (Corpus of Linguistic Acceptability) - Grammatical Acceptibility\n",
    "    Measures the ability to determine if an English sentence is linguistically acceptable. \n",
    "\n",
    "##### SST-2 (Stanford Sentiment Treebank) - Sentiment Analysis\n",
    "    Consists of sentences from movie reviews and human annotations about their sentiment. Analyzes sentiment in movie reviews.  \n",
    "\n",
    "##### MRPC (Microsoft Research Paraphrase Corpus) - Paraphrase Identification\n",
    "    Focuses on identifying whether two sentences are paraphrases of each other.  \n",
    "\n",
    "##### STS-B (Semantic Textual Similarity Benchmark) - Semantic Textual Similarity\n",
    "    Involves determining how similar two sentences are in terms of semantic content.  \n",
    "\n",
    "##### QQP (Quora Question Pairs) - Question Pairs Equivalence\n",
    "    Aims to identify whether two questions asked on Quora are semantically equivalent. Evaluates semantic equivalence of questions. \n",
    "\n",
    "##### MNLI (Multi-Genre Natural Language Inference) - Natural Language Inference\n",
    "    Consists of sentence pairs labeled for textual entailment across multiple genres of text. Assesses the relationship between sentence pairs.  \n",
    "\n",
    "##### QNLI (Question Natural Language Inference) - Question Answering Inference\n",
    "    Involves determining whether the content of a paragraph or a context sentence contains the answer to a question.\n",
    "\n",
    "##### RTE (Recognizing Textual Entailment) - Textual Entailment Recognition\n",
    "    Requires understanding whether one sentence entails another.\n",
    "\n",
    "###### WNLI (Winograd Natural Language Inference) - Pronoun Disambiguation\n",
    "    Tests a system's reading comprehension by having it determine the correct referent of a pronoun in a sentence, where understanding depends on contextual information provided by specific words or phrases. Tests reading comprehension by resolving pronouns in context.  \n",
    "\n",
    "\n",
    "Semantic Equivalence: When different phrases or sentences convey the same meaning or idea.\n",
    "\n",
    "Textual Entailment: The relationship between text fragments where one fragment follows logically from the other."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11830187-0072-4b0c-a64f-2ddcb1ea7ba3",
   "metadata": {},
   "source": [
    "#### Importance of Evaluation \n",
    "The GLUE benchmark allows researchers to compare the performance of different models on standardized tasks, facilitating advancements in natural language processing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9dd8e5-7453-4c4c-a7de-711fa1dcf9cc",
   "metadata": {},
   "source": [
    "### SuperGlue\n",
    "SuperGlue is designed as a successor to the original GLUE benchmark. It's a more advanced benchmark aimed at presenting even more challenging language understanding tasks for AI models. Created to push the boundaries of what AI can understand and process in natural language, SuperGlue emerged as models began to achieve human parity on the GLUE benchmark. It also features a public leaderboard, facilitating the direct comparison of models and enabling the tracking of progress over time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7f28b8-df58-4313-90d0-2b3b40ac2e12",
   "metadata": {},
   "source": [
    "#### SuperGlue Tasks / Benchmarks\n",
    "##### BoolQ (Boolean Questions)\n",
    "Involves answering a yes/no question based on a short passage.\n",
    "\n",
    "##### CB (Commitment Bank)\n",
    "    Tests understanding of entailment and contradiction in a three-sentence format.\n",
    "\n",
    "##### COPA (Choice of Plausible Alternatives)\n",
    "    Measures causal reasoning by asking for the cause/effect of a given sentence.\n",
    "\n",
    "##### MultiRC (Multi-Sentence Reading Comprehension)\n",
    "    Involves answering questions about a paragraph where each question may have multiple correct answers.\n",
    "\n",
    "##### ReCoRD (Reading Comprehension with Common Senese Reasoning)\n",
    "    Requires selecting the correct named entity from a passage to fill in the blank of a question.\n",
    "\n",
    "##### RTE (Recognizing Textual Entailment)\n",
    "    Involves identifying whether a sentence entails, contradicts, or is neutral towards another sentence.\n",
    "\n",
    "##### WiC (Words in Context)\n",
    "    Tests understanding of word sense disambiguation in different contexts.\n",
    "\n",
    "##### WSC (Winograd Schema Challenge)\n",
    "    Focuses on resolving coreference resolution within a sentence, often requiring commonsense reasoning.\n",
    "\n",
    "##### AX-b (Broad Coverage Diagnostics)\n",
    "    A diagnostic set to evaluate model performance on a broad range of linguistic phenomena.\n",
    "    \n",
    "##### AX-g (Winogender Schema Diagnostics)\n",
    "    Tests for the presence of gender bias in automated coreference resolution systems.\n",
    "\n",
    "\n",
    "Coreference Resolution: This is figuring out when different words or phrases in a text, like the pronoun she and the president, refer to the same person or thing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ecdf16-7c97-4ca3-b8b8-e09de7578967",
   "metadata": {},
   "source": [
    "## Data used for Training LLMs\n",
    "Generative AI, specifically Large Language Models (LLMs), rely on a rich mosaic of data sources to fine-tune their linguistic skills. These sources include web content, academic writings, literary works, and multilingual texts, among others. By engaging with a variety of data types, such as scientific papers, social media posts, legal documents, and even conversational dialogues, LLMs become adept at comprehending and generating language across many contexts, enhancing their ability to provide relevant and accurate information.\n",
    "\n",
    "### Diverse Data Sources\n",
    "- Websites: Content from various online sources, including articles and blogs, helps models learn both formal and informal language.\n",
    "- Scientific Papers: Academic texts provide technical language and complex concepts, which are useful for expert-level queries.\n",
    "- Encyclopedias: Factual entries give models a basis for general knowledge across many topics.\n",
    "- Books and Literature: Classic and modern literature enriches the model's vocabulary and understanding of complex sentence structures.\n",
    "- Conversational Data: Transcripts from dialogues and chatbots help models grasp nuances in dialogue and colloquial speech.\n",
    "- Social Media Posts: This data helps models understand current linguistic trends and informal communication styles.\n",
    "- Legal Documents: These texts train models to comprehend formal language and complex structures.\n",
    "- Multilingual Texts: Including texts in various languages helps models understand and generate language across different linguistic contexts.\n",
    "\n",
    "\n",
    "Preprocessing: This is the process of preparing and cleaning data to ensure quality before it is used to train a machine learning model. It might involve removing errors, irrelevant information, or anonymizing information or formatting the data in a way that the model can easily learn from it.\n",
    "\n",
    "Fine-tuning: After a model has been pre-trained on a large dataset, fine-tuning is an additional training step where the model is further refined with specific data to improve its performance on a particular type of task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631c8cf3-7f74-4889-acd5-8e2202cadc65",
   "metadata": {},
   "source": [
    "### Data Scale and Volume\n",
    "The scale of data for Large Language Models (LLMs) is tremendously vast, involving datasets that could equate to millions of books. The sheer size is pivotal for the model's understanding and mastery of language through exposure to diverse words and structures.\n",
    "\n",
    "Common Crawl: An open repository of web crawl data. Essentially, it is a large collection of content from the internet that is gathered by automatically scraping the web."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a082e15f-0b45-45f7-992e-5b6e40a6b86e",
   "metadata": {},
   "source": [
    "### Biases in Training Data\n",
    "Biases in training data deeply influence the outcomes of AI models, reflecting societal issues that require attention. Ways to approach this challenge include promoting diversity in development teams, seeking diverse data sources, and ensuring continued vigilance through bias detection and model monitoring.\n",
    "\n",
    "#### Types of Bias\n",
    "##### Selection Bias - Biased data selection\n",
    "    When the data used to train an AI model does not accurately represent the whole population or situation by virtue of the selection process leading to a skewed result, e.g. those choosing the data will tend to choose dataset their are aware of.\n",
    "    \n",
    "##### Historical Bias - Caused by historical prejudices in data\n",
    "    Prejudices and societal inequalities of the past that are reflected in the data, influencing the AI in a way that perpetuates these outdated beliefs.\n",
    "    \n",
    "##### Confirmation Bias - caused by data with pre-existing beliefs\n",
    "    Arises when data is selected to confirm pre-existing beliefs, further skewing the model's understanding.\n",
    "\n",
    "#### Effects of Biased Data\n",
    "##### Discriminatory Outcomes\n",
    "    Unfair results produced by AI that disadvantage certain groups, often due to biases in the training data or malicious actors, such as biased hiring practices or loan approvals.\n",
    "    \n",
    "##### Echo Chambers\n",
    "    Situations where biased AI reinforces and amplifies existing biases, leading to a narrow and distorted sphere of information. This can create feedback loops that reinforce existing biases, limiting diverse perspectives.\n",
    "\n",
    "##### Misrepresentation\n",
    "    Certain groups may be underrepresented or misrepresented in the outputs of AI models.\n",
    "\n",
    "#### Mitigating Bias\n",
    "##### Organizational Diversity\n",
    "    Ensuring that the teams involved are diverse\n",
    "    - Fair models = fair teams, companies, and society\n",
    "\n",
    "##### Diverse Data Collection\n",
    "    Actively seeking out diverse data sources\n",
    "\n",
    "##### Bias Detection and Correction - employing algorithms and human oversight\n",
    "    Processes and algorithms designed to identify and remove biases from data before it's used to train AI models.\n",
    "\n",
    "##### Transparency and Accountability - being transparent about sources and nature of training data \n",
    "    Openness about how AI models are trained and the nature of their data, ensuring that developers are answerable for their AI's performance and impact.\n",
    "\n",
    "##### Continuous Monitoring\n",
    "    Regularly testing and updating the models.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8aedb91-d548-4bd1-a8f0-dd2b80409359",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "787a0314-a46f-43e9-b54b-458399cc64d8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1a4bc13b-5d35-460d-8aa7-c50b7d7a8eca",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2291704e-ebce-48cb-9ac9-c8aa199f6be6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "308ba0d0-6cb7-4621-bf60-b9bcdf18f4bb",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25fd742-c640-4e51-b69a-065a0449f351",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
