{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c3590ee-a3c9-4e27-a163-9cff9a5a5352",
   "metadata": {},
   "source": [
    "# Language Model (LM)\n",
    "Language Model gives the probability distribution over a sequence of tokens.\n",
    "\n",
    "<img src=\"../Images/LM.png\" alt=\"LMs can 'Generate' Text\" width=\"750\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1e1056-81a8-45eb-bc67-f3aec4878486",
   "metadata": {},
   "source": [
    "# 'Large' Language Models\n",
    "The 'Large' in terms of model's size (number of parameters) and massive size of training dataset.\n",
    "\n",
    "LLMs are neural networks trained on enormous volumes of text data. Instead of relying on hard-coded rules, they learn the statistical patterns, syntactic structures, and even some semantic nuances of language by processing billions of words.\n",
    "\n",
    "LLMs learn by predicting the next word in a sentence (a task known as next-token prediction) based on the context provided by preceding words. This self-supervised training method allows them to “learn” language without needing manually labeled examples. Most modern LLMs use the transformer model—a deep neural network that employs attention mechanisms to capture relationships between all words in a sentence simultaneously. This design enables them to consider long-range dependencies and context more efficiently than earlier models like recurrent neural networks. Instead of processing words as raw text, LLMs convert them into high-dimensional vectors (embeddings). These embeddings encapsulate the “meaning” of words based on their usage in large corpora, so words used in similar contexts have similar numerical representations.\n",
    "\n",
    "<img src=\"../Images/LLM.png\" alt=\"LLMs in AI Landscape\" width=\"750\">\n",
    "\n",
    "[LLM - Educative](https://www.educative.io/courses/vector-databases-for-llms/introduction-to-large-language-models-llms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2efd87-40bc-4524-a5c8-3b6d4b6af58a",
   "metadata": {},
   "source": [
    "## LLMs Evolution\n",
    "\n",
    "[Evolution of LLMs](https://synthedia.substack.com/p/a-timeline-of-large-language-model)\n",
    "\n",
    "- Google introduced Transformer architecture in 2017 in \"Attention Is All You Need\" paper. It also released BERT model in 2018 for language translation which achieved SOTA (State Of The Art) on 11 NLP tasks. But due to its large parameters size at that time, small models like DistilBERT, TinyBERT, MobileBERT based on BERT were proposed. BERT is an Encoder-only model. This started the beginning of use of Transformer as Language Representation Models.\n",
    "\n",
    "- OpenAI published \"Improving Language Understanding by Generative Pre-Training\" in 2018 and released GPT-1(117M, 512 tokens) model which used Decoder-only architecture. It also introduced the idea of generative pre-training over large corpus.\n",
    "\n",
    "- OpenAI published \"Language Models are Unsupervised Multitask Learners\" paper in 2019 and also released GPT-2(1.5B, 1024 tokens) model.\n",
    "\n",
    "- Google developed T5 (Text-To-Text Transfer Transformer) model in 2019. It used Encoder-Decoder architecture.\n",
    "\n",
    "- Meta released RoBERTa model and published \"RoBERTa: A Robustly Optimized Pretraining Approach\" paper in 2019. It found that BERT was significantly undertrained.\n",
    "\n",
    "- Meta also released XLM model along with \"Cross-lingual Language Model Pretraining\" paper in 2019. It proposed methods to learn cross-lingual language models (XLMs). It obtained SOTA on cross-lingual classification, and unsupervised and supervised machine translation.\n",
    "\n",
    "- OpenAI released GPT-3(175B) model and \"Language Models are Few-Shot Learners\" in 2020. It observed the phenomana of In-context learning. OpenAI stopped open-sourcing.\n",
    "\n",
    "- Google released PalM model and \"PaLM: Scaling Language Modeling with Pathways\" paper in 2022. It stopped open-sourcing.\n",
    "\n",
    "- Meta released OPT model and \"OPT: Open Pre-trained Transformer Language Models\" in 2022. It is a suite of Decoder-only pre-trained transformers ranging from 125M to 175B parameters. It promotes open-sourcing.\n",
    "\n",
    "<img src=\"../Images/2023Models.png\" alt=\"Models in 2023\" width=\"750\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63af8ae4-2fd2-41ac-bb6b-eb7bfb10ec48",
   "metadata": {},
   "source": [
    "## Why the separate focus on LLMs\n",
    "LLMs show emergent capabilities, not observed previously in ‘small’ LMs.\n",
    "- In-context learning: A pre-trained language model can be guided with only prompts to perform different tasks (without separate task-specific fine-tuning).\n",
    "- In-context learning is an example of emergent behavior.\n",
    "\n",
    "LLMs are widely adopted in real-world.\n",
    "- Research: LLMs have transformed NLP research world, achieving state-of-the-art performance across a wide range of tasks such as sentiment classification, question answering, summarization, and machine translation.\n",
    "- Industry: Here is a very incomplete list of some high profile large language models that are being used in production systems:\n",
    "  - [Google Search](https://blog.google/products/search/search-language-understanding-bert/) (BERT)\n",
    "  -  [Facebook content moderation](https://ai.meta.com/blog/harmful-content-can-evolve-quickly-our-new-ai-system-adapts-to-tackle-it/) (XLM)\n",
    "  - [Microsoft’s Azure OpenAI Service](https://news.microsoft.com/source/features/innovation/new-azure-openai-service/) (GPT-3/3.5/4)\n",
    "\n",
    "With tremendous capabilities, LLMs’ usage also carries various risks.\n",
    "- Reliability & Disinformation: LLMs often hallucinate – generate responses that seem correct, but are not factually correct.\n",
    "  - Significant challenge for high-stakes applications like healthcare\n",
    "- Social bias: Most LLMs show performance disparities across demographic groups, and their predictions can enforce stereotypes.\n",
    "  - P(He is a doctor) > P(She is a doctor.)\n",
    "  - Training data contains inherent bias\n",
    "- Toxicity: LLMs can generate toxic/hateful content.\n",
    "  - Trained on a huge amount of Internet data (e.g., Reddit), which inevitably contains offensive content\n",
    "  - Challenge for applications such as writing assistants or chatbots\n",
    "- Security: LLMs are trained on a scrape of the public Internet - anyone can put up a website that can enter the training data.\n",
    "  - An attacker can perform a data poisoning attack. <br/>\n",
    "Credits: https://stanford-cs324.github.io/winter2022/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d753a3-f49d-4e05-84ff-237068dd4b89",
   "metadata": {},
   "source": [
    "## Buildiing blocks of LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "529e792b-4109-4d0d-bf4c-0cbd94ca2d59",
   "metadata": {},
   "source": [
    "### Text Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055f3509-c1cc-4979-b6e2-c14107087b32",
   "metadata": {},
   "source": [
    "### Text Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f11546-ee50-4052-ba02-c51d4b9741ae",
   "metadata": {},
   "source": [
    "### Pre-training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac34552d-471f-46f4-a310-8e64ce58b495",
   "metadata": {},
   "source": [
    "### Fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ac1003-5895-4d51-8b30-28b121c4f5e6",
   "metadata": {},
   "source": [
    "## Challenges of Language Modeling\n",
    "- Sequence matters - words in different place changes the meaning of the sentence.\n",
    "- Cotext modeling - same word means different based on its use and context.\n",
    "- Long-range dependency - recognize and connect distant words in a sentence and maintain a long-range dependency between them\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3aa0f3-8613-408c-859e-7e356c491086",
   "metadata": {},
   "source": [
    "# Multimodal\n",
    "Multi means many and modal means modes or types. These models can process and generate information across different data types, such as text, audio, video, and images, in contrast to non-multimodal, which work with only one of the modes, such as text only.\n",
    "\n",
    "Visual question answering is one such multimodal application. LLMs can generate meaningful and contextually relevant answers to questions about visual content, such as identifying objects, understanding relationships between them, and describing scenes. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53a69ff-838a-4940-a382-26ed690a3cd7",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
