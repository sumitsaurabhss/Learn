{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d688ed05",
   "metadata": {},
   "source": [
    "## ML pipeline\n",
    "An ML pipeline is a portable and extensible description of an MLOps workflow as a series of steps called pipeline tasks. Each task performs a specific step in the workflow to train and/or deploy an ML model.\n",
    "\n",
    "An ML pipeline is a directed acyclic graph (DAG) of containerized pipeline tasks that are interconnected using input-output dependencies. Each task can be authored either in Python or as a prebuilt container images.\n",
    "\n",
    "Define the Pipeline as a DAG using either the Kubeflow Pipelines SDK or the TFX SDK, compile it to its YAML for intermediate representation, and then run the pipeline. By default, pipeline tasks run in parallel. You can link the tasks to execute them in series."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e0fccc",
   "metadata": {},
   "source": [
    "Consider an ML pipeline with the following steps:\n",
    "\n",
    "* **Prepare data**: Prepare or preprocess training data.\n",
    "    - Input (from tasks within the same ML pipeline): None.\n",
    "    - Output: Prepared or preprocessed training data.\n",
    "\n",
    "* **Train model**: Use the prepared training data to train a model.\n",
    "    - Input: Prepared or preprocessed training data from pipeline task Prepare data.\n",
    "    - Output: Trained model.\n",
    "\n",
    "* **Evaluate model**: Evaluate the trained model.\n",
    "    - Input: Trained model from pipeline task Train model.\n",
    "\n",
    "* **Deploy**: Deploy the trained model for predictions.\n",
    "    - Input: Trained model from pipeline task Train model.\n",
    "\n",
    "When you compile your ML pipeline, the pipelines SDK you're using (Kubeflow Pipelines or TFX) analyzes the data dependencies between these tasks and creates the following workflow DAG:\n",
    "\n",
    "* **Prepare data** doesn't rely on other tasks within the same ML pipeline for inputs. Therefore, it can be the first step in the ML pipeline, or run concurrently with other tasks.\n",
    "* **Train model** relies on **Prepare data** for inputs. Therefore, it occurs after **Prepare data**.\n",
    "* **Evaluate** and **Deploy** both depend on the trained model. Therefore, they can run concurrently, but after **Train model**.\n",
    "\n",
    "When you run your ML pipeline, Vertex AI Pipelines executes these tasks in the sequence described in the DAG."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db9c5ec",
   "metadata": {},
   "source": [
    "### Pipeline tasks and components\n",
    "A pipeline task is an instantiation of a pipeline component with specific inputs. While defining an ML pipeline, you can interconnect multiple tasks to form a DAG, by routing the outputs of one pipeline task to the inputs for the next pipeline task in the ML workflow. You can also use the inputs for the ML pipeline as the inputs for a pipeline task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ecee0b3-0006-45f9-aadb-fa1c38b9a72a",
   "metadata": {},
   "source": [
    "#### Pipeline component\n",
    "A pipeline component is a self-contained set of code that performs a specific step of an ML workflow. A component typically consists of the following:\n",
    "- **Inputs**: A component might have one or more input parameters and artifacts.\n",
    "- **Outputs**: Every component has one or more output parameters or artifacts.\n",
    "- **Logic**: This is the component's executable code. For containerized components, the logic also contains the definition of the environment, or container image, where the component runs.\n",
    "\n",
    "Components are the basis of defining tasks in an ML pipeline. To define pipeline tasks, you can either use predefined Google Cloud Pipeline Components or create your own custom components. Use predefined Google Cloud Pipeline Components if you want to use features of Vertex AI, such as AutoML, in your pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622e6287-696d-467f-b9cc-a2f640996a63",
   "metadata": {},
   "source": [
    "#### Pipeline task\n",
    "A pipeline task is the instantiation of a pipeline component and performs a specific step in your ML workflow. You can author ML pipeline tasks either using Python or as prebuilt container images.\n",
    "\n",
    "Within a task, you can build on the on-demand compute capabilities of Vertex AI with Kubernetes to scalably execute your code, or delegate your workload to another execution engine, such as BigQuery, Dataflow, or Dataproc Serverless."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a527b65",
   "metadata": {},
   "source": [
    "### Lifecycle of an ML pipeline\n",
    "From definition to execution and monitoring, the lifecycle of an ML pipeline comprises the following high-level stages:\n",
    "- **Define**: The process of defining an ML pipeline and its task is also called building a pipeline. In this stage, you need to perform the following steps:\n",
    "    - Choose an ML framework: Vertex AI Pipelines supports ML pipelines defined using the TFX or Kubeflow Pipelines framework.\n",
    "    - Define pipeline tasks and configure pipeline.\n",
    "- **Compile**: In this stage, you need to perform the following steps:\n",
    "    - Generate your ML pipeline definition in a compiled YAML file for intermediate representation, which you can use to run your ML pipeline.\n",
    "    - Optional: You can upload the compiled YAML file as a pipeline template to a repository and reuse it to create ML pipeline runs.\n",
    "- **Run**: Create an execution instance of your ML pipeline using the compiled YAML file or a pipeline template. The execution instance of a pipeline definition is called a pipeline run.\n",
    "    - You can create a one-time occurrence of a pipeline run or use the scheduler API to create recurring pipeline runs from the same ML pipeline definition. You can also clone an existing pipeline run.\n",
    "- **Monitor, visualize, and analyze runs**: After you create a pipeline run, you can do the following to monitor the performance, status, and costs of pipeline runs:\n",
    "    - Configure email notifications for pipeline failures.\n",
    "    - Use Cloud Logging to create log entries for monitoring events.\n",
    "    - Visualize, analyze, and compare pipeline runs.\n",
    "    - Use Cloud Billing export to BigQuery to analyze pipeline run costs.\n",
    "\n",
    "- **Optional: stop or delete pipeline runs**: There is no restriction on how long you can keep a pipeline run active. You can optionally do the following:\n",
    "    - Stop a pipeline run.\n",
    "    - Pause or resume a pipeline run schedule.\n",
    "    - Delete an existing pipeline template, pipeline run, or pipeline run schedule."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "844bf5c8",
   "metadata": {},
   "source": [
    "### Pipeline run\n",
    "A pipeline run is an execution instance of your ML pipeline definition. Each pipeline run is identified by a unique run name. Using Vertex AI Pipelines, you can create an ML pipeline run in the following ways:\n",
    "- Use the compiled YAML definition of a pipeline\n",
    "- Use a pipeline template from the Template Gallery"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d709ac",
   "metadata": {},
   "source": [
    "### Track the lineage of ML artifacts\n",
    "A pipeline run contains several artifacts and parameters, including pipeline metadata. To understand changes in the performance or accuracy of your ML system, you need to analyze the metadata and the lineage of ML artifacts from your ML pipeline runs. The lineage of an ML artifact includes all the factors that contributed to its creation, along with metadata and references to artifacts derived from it.\n",
    "\n",
    "Lineage graphs help you analyze upstream root cause and downstream impact. Each pipeline run produces a lineage graph of parameters and artifacts that are input into the run, materialized within the run, and output from the run. Metadata that composes this lineage graph is stored in Vertex ML Metadata. This metadata can also be synced to Dataplex.\n",
    "\n",
    "- **Use Vertex ML Metadata to track pipeline artifact lineage**\n",
    "  When you run a pipeline using Vertex AI Pipelines, all parameters and artifact metadata consumed and generated by the pipeline are stored in Vertex ML Metadata. Vertex ML Metadata is a managed implementation of the ML Metadata library in TensorFlow, and supports registering and writing custom metadata schemas. When you create a pipeline run in Vertex AI Pipelines, metadata from the pipeline run is stored in the default metadata store for the project and region where you execute the pipeline.\n",
    "\n",
    "- **Use Dataplex to track pipeline artifact lineage**\n",
    "  Dataplex is a global and cross-project data fabric integrated with multiple systems within Google Cloud, such as Vertex AI, BigQuery, and Cloud Composer. Within Dataplex, you can search for a pipeline artifact and view its lineage graph. Note that to prevent artifact conflicts, any resource catalogued in Dataplex is identified with a fully qualified name (FQN)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88b568b",
   "metadata": {},
   "source": [
    "## Interfaces to define a pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e519bd",
   "metadata": {},
   "source": [
    "### Kubeflow Pipelines (KFP) SDK\n",
    "Use KFP for all use cases where you don't need to use TensorFlow Extended to process huge amounts of structured or text data. Vertex AI Pipelines supports KFP SDK v1.8 or later.\n",
    "\n",
    "When you use the KFP SDK, you can define your ML workflow by building custom components and also by reusing prebuilt components, such as the Google Cloud Pipeline Components. Vertex AI Pipelines supports Google Cloud Pipeline Components SDK v2 or later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2980392d",
   "metadata": {},
   "source": [
    "### TensorFlow Extended (TFX) SDK\n",
    "TFX if you use TensorFlow Extended in your ML workflow to process terabytes of structured or text data. Vertex AI Pipelines supports TFX SDK v0.30.0 or later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7374501f",
   "metadata": {},
   "source": [
    "## Interfaces to run a pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a111da5",
   "metadata": {},
   "source": [
    "### REST API\n",
    "To create a pipeline run using REST, use the `Pipelines` service API. This API uses the `projects.locations.pipelineJobs` REST resource."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ea2e54",
   "metadata": {},
   "source": [
    "### SDK Clients\n",
    "Vertex AI Pipelines lets you create pipeline runs using the Vertex AI SDK for Python or client libraries.\n",
    "\n",
    "**Vertex AI SDK for Python**  \n",
    "The Vertex AI SDK for Python (`aiplatform`) is the recommended SDK for programmatically working with the `Pipelines` service API. `google.cloud.aiplatform.PipelineJob`.\n",
    "\n",
    "**Client libraries**  \n",
    "Client libraries are programmatically Generated API Clients (GAPIC) SDKs. Vertex AI Pipelines supports the following client libraries:\n",
    "- Python (`aiplatform v1` and `v1beta1`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd55c67",
   "metadata": {},
   "source": [
    "### Google Cloud console (GUI)\n",
    "Google Cloud console is the recommended way for reviewing and monitoring your pipeline runs. You can also perform other tasks using the Google Cloud console, such as creating, deleting and cloning pipeline runs, accessing the Template Gallery, and retrieving the billing label for a pipeline run."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14551bbd",
   "metadata": {},
   "source": [
    "## Build a pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d419291",
   "metadata": {},
   "source": [
    "### Kubeflow Pipelines DSL package\n",
    "The `kfp.dsl` package contains the domain-specific language (DSL) that you can use to define and interact with pipelines and components.\n",
    "\n",
    "Kubeflow pipeline components are factory functions that create pipeline steps. Each component describes the inputs, outputs, and implementation of the component. For example, in the code sample below, `ds_op` is a component.\n",
    "\n",
    "Components are used to create pipeline steps. When a pipeline runs, steps are executed as the data they depend on becomes available. For example, a training component could take a CSV file as an input and use it to train a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03997cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "from google.cloud import aiplatform\n",
    "from google_cloud_pipeline_components.v1.dataset import ImageDatasetCreateOp\n",
    "from google_cloud_pipeline_components.v1.automl.training_job import AutoMLImageTrainingJobRunOp\n",
    "from google_cloud_pipeline_components.v1.endpoint import EndpointCreateOp, ModelDeployOp\n",
    "\n",
    "# The Google Cloud project that this pipeline runs in.\n",
    "project_id = PROJECT_ID\n",
    "\n",
    "# Specify a Cloud Storage URI that your pipelines service account can access.\n",
    "# The artifacts of your pipeline runs are stored within the pipeline root.\n",
    "pipeline_root_path = PIPELINE_ROOT\n",
    "\n",
    "# Define the workflow of the pipeline.\n",
    "@kfp.dsl.pipeline(\n",
    "    name=\"automl-image-training-v2\",\n",
    "    pipeline_root=pipeline_root_path)\n",
    "def pipeline(project_id: str):\n",
    "    # The first step of your workflow is a dataset generator.\n",
    "    # This step takes a Google Cloud Pipeline Component, providing the necessary\n",
    "    # input arguments, and uses the Python variable `ds_op` to define its\n",
    "    # output. Note that here the `ds_op` only stores the definition of the\n",
    "    # output but not the actual returned object from the execution. The value\n",
    "    # of the object is not accessible at the dsl.pipeline level, and can only be\n",
    "    # retrieved by providing it as the input to a downstream component.\n",
    "    ds_op = ImageDatasetCreateOp(\n",
    "        project=project_id,\n",
    "        display_name=\"flowers\",\n",
    "        gcs_source=\"gs://cloud-samples-data/vision/automl_classification/flowers/all_data_v2.csv\",\n",
    "        import_schema_uri=aiplatform.schema.dataset.ioformat.image.single_label_classification,\n",
    "    )\n",
    "\n",
    "    # The second step is a model training component. It takes the dataset\n",
    "    # outputted from the first step, supplies it as an input argument to the\n",
    "    # component (see `dataset=ds_op.outputs[\"dataset\"]`), and will put its\n",
    "    # outputs into `training_job_run_op`.\n",
    "    training_job_run_op = AutoMLImageTrainingJobRunOp(\n",
    "        project=project_id,\n",
    "        display_name=\"train-iris-automl-mbsdk-1\",\n",
    "        prediction_type=\"classification\",\n",
    "        model_type=\"CLOUD\",\n",
    "        dataset=ds_op.outputs[\"dataset\"],\n",
    "        model_display_name=\"iris-classification-model-mbsdk\",\n",
    "        training_fraction_split=0.6,\n",
    "        validation_fraction_split=0.2,\n",
    "        test_fraction_split=0.2,\n",
    "        budget_milli_node_hours=8000,\n",
    "    )\n",
    "\n",
    "    # The third and fourth step are for deploying the model.\n",
    "    create_endpoint_op = EndpointCreateOp(\n",
    "        project=project_id,\n",
    "        display_name = \"create-endpoint\",\n",
    "    )\n",
    "\n",
    "    model_deploy_op = ModelDeployOp(\n",
    "        model=training_job_run_op.outputs[\"model\"],\n",
    "        endpoint=create_endpoint_op.outputs['endpoint'],\n",
    "        automatic_resources_min_replica_count=1,\n",
    "        automatic_resources_max_replica_count=1,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026b2856",
   "metadata": {},
   "source": [
    "> The pipeline root can be set as an argument of the @kfp.dsl.pipeline annotation on the pipeline function, or it can be set when you call create_run_from_job_spec to create a pipeline run."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d86dc4",
   "metadata": {},
   "source": [
    "### Compile your pipeline into a YAML file\n",
    "After the workflow of your pipeline is defined, you can proceed to compile the pipeline into YAML format. The YAML file includes all the information for executing your pipeline on Vertex AI Pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508d3bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp import compiler\n",
    "\n",
    "compiler.Compiler().compile(\n",
    "    pipeline_func=pipeline,     # The name of your pipeline's function.\n",
    "    # The path to where to store your compiled pipeline.\n",
    "    package_path='image_classif_pipeline.yaml'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e5b410",
   "metadata": {},
   "source": [
    "### Submit your pipeline run\n",
    "After the workflow of your pipeline is compiled into the YAML format, you can use the Vertex AI Python client to submit and run your pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420c6904",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.cloud.aiplatform as aip\n",
    "\n",
    "# Before initializing, make sure to set the GOOGLE_APPLICATION_CREDENTIALS\n",
    "# environment variable to the path of your service account.\n",
    "aip.init(\n",
    "    project=project_id,\n",
    "    location=PROJECT_REGION,  # The region that this pipeline runs in.\n",
    ")\n",
    "\n",
    "# Prepare the pipeline job\n",
    "job = aip.PipelineJob(\n",
    "    display_name=\"automl-image-training-v2\",\n",
    "    template_path=\"image_classif_pipeline.yaml\",\n",
    "    pipeline_root=pipeline_root_path,\n",
    "    parameter_values={\n",
    "        'project_id': project_id\n",
    "    }\n",
    ")\n",
    "\n",
    "job.submit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7855c748",
   "metadata": {},
   "source": [
    "In the preceding example:\n",
    "- A Kubeflow pipeline is defined as a Python function. The function is annotated with the `@kfp.dsl.pipeline` decorator, which specifies the pipeline's name and root path. The pipeline root path is the location where the pipeline's artifacts are stored.\n",
    "- The pipeline's workflow steps are created using the Google Cloud Pipeline Components. By using the outputs of a component as an input of another component, you define the pipeline's workflow as a graph. For example: `training_job_run_op` depends on the `dataset` output of `ds_op`.\n",
    "- You compile the pipeline using `kfp.compiler.Compiler`.\n",
    "- You create a pipeline run on Vertex AI Pipelines using the Vertex AI Python client. When you run a pipeline, you can override the pipeline name and the pipeline root path. Pipeline runs can be grouped using the pipeline name. Overriding the pipeline name can help you distinguish between production and experimental pipeline runs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fee8cbd",
   "metadata": {},
   "source": [
    "## Building Kubeflow pipelines\n",
    "Use the following process to build a pipeline.\n",
    "1. Design your pipeline as a series of components. To promote reusability, each component should have a single responsibility. Whenever possible, design your pipeline to reuse proven components such as the Google Cloud Pipeline Components.\n",
    "2. Build any custom components that are required to implement your ML workflow using Kubeflow Pipelines SDK. Components are self-contained sets of code that perform a step in your ML workflow. Use the following options to create your pipeline components.\n",
    "    - Package your component's code as a container image. This option lets you include code in your pipeline that was written in any language that can be packaged as a container image.\n",
    "    - Implement your component's code as a standalone Python function and use the Kubeflow Pipelines SDK to package your function as a component. This option makes it easier to build Python-based components.\n",
    "3. Build your pipeline as a Python function.\n",
    "4. Use the Kubeflow Pipelines SDK compiler to compile your pipeline.\n",
    "5. Run your pipeline using Google Cloud console or Python."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752408a4",
   "metadata": {},
   "source": [
    "> If you have a pipeline template or definition that references a container with security vulnerabilities, you should do the following:  \n",
    "    1. Install the latest patched version of the SDK.  \n",
    "    2. Rebuild and recompile your pipeline template or definition.  \n",
    "    3. Re-upload the template or definition to Artifact Registry or Cloud Storage  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc257e17",
   "metadata": {},
   "source": [
    "## Run a pipeline \n",
    "Vertex AI Pipelines lets you run machine learning (ML) pipelines that were built using the Kubeflow Pipelines SDK or TensorFlow Extended in a serverless manner. You can also create pipeline runs using prebuilt templates in the **Template Gallery**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecdd91bb",
   "metadata": {},
   "source": [
    "### Create a pipeline run\n",
    "**Set up authentication**  \n",
    "To set up authentication, you must create a service account key, and set an environment variable for the path to the service account key.\n",
    "\n",
    "`GOOGLE_APPLICATION_CREDENTIALS=\"[PATH]\"`      # Replace *[PATH]* with the path of the JSON file that contains your service account key."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d58721",
   "metadata": {},
   "source": [
    "#### Run a pipeline\n",
    "Running a Vertex AI `PipelineJob` requires you to create a `PipelineJob` object, and then invoke the `submit` method.\n",
    "\n",
    "**Special input types supported by KFP**\n",
    "While creating a pipeline run, you can also pass the following placeholders supported by the KFP SDK as inputs:\n",
    "- `{{$.pipeline_job_name_placeholder}}`\n",
    "- `{{$.pipeline_job_resource_name_placeholder}}`\n",
    "- `{{$.pipeline_job_id_placeholder}}`\n",
    "- `{{$.pipeline_task_name_placeholder}}`\n",
    "- `{{$.pipeline_task_id_placeholder}}`\n",
    "- `{{$.pipeline_job_create_time_utc_placeholder}}`\n",
    "- `{{$.pipeline_root_placeholder}}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba37366",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "\n",
    "job = aiplatform.PipelineJob(display_name=DISPLAY_NAME,\n",
    "                             template_path=COMPILED_PIPELINE_PATH,\n",
    "                             job_id=JOB_ID,\n",
    "                             pipeline_root=PIPELINE_ROOT_PATH,\n",
    "                             parameter_values=PIPELINE_PARAMETERS,\n",
    "                             enable_caching=ENABLE_CACHING,\n",
    "                             encryption_spec_key_name=CMEK,\n",
    "                             labels=LABELS,\n",
    "                             credentials=CREDENTIALS,\n",
    "                             project=PROJECT_ID,\n",
    "                             location=LOCATION,\n",
    "                             failure_policy=FAILURE_POLICY)\n",
    "\n",
    "job.submit(service_account=SERVICE_ACCOUNT,\n",
    "           network=NETWORK)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374b664f",
   "metadata": {},
   "source": [
    "Replace the following:\n",
    "- ***DISPLAY_NAME***: The name of the pipeline, this will show up in the Google Cloud console.\n",
    "- ***COMPILED_PIPELINE_PATH***: The path to your compiled pipeline YAML file. It can be a local path or a Cloud Storage URI.  \n",
    "  Optional: To specify a particular version of a compiled pipeline, include the version tag in any one of the following formats:\n",
    "  - ***`COMPILED_PIPELINE_PATH:TAG`***, where ***TAG*** is the version tag.\n",
    "  - ***`COMPILED_PIPELINE_PATH@SHA256_TAG`***, where ***SHA256_TAG*** is the `sha256` hash value of the pipeline version.\n",
    "- ***JOB_ID***: (optional) A unique identifier for this pipeline run. If the job ID is not specified, Vertex AI Pipelines creates a job ID for you using the pipeline name and the timestamp of when the pipeline run was started.\n",
    "- ***PIPELINE_ROOT_PATH***: (optional) To override the pipeline root path specified in the pipeline definition, specify a path that your pipeline job can access, such as a Cloud Storage bucket URI.\n",
    "- ***PIPELINE_PARAMETERS***: (optional) The pipeline parameters to pass to this run. For example, create a `dict()` with the parameter names as the dictionary keys and the parameter values as the dictionary values.\n",
    "- ***ENABLE_CACHING***: (optional) Specifies if this pipeline run uses execution caching. Execution caching reduces costs by skipping pipeline tasks where the output is known for the current set of inputs. If the enable caching argument is not specified, execution caching is used in this pipeline run.\n",
    "- ***CMEK***: (optional) The name of the customer-managed encryption key that you want to use for this pipeline run.\n",
    "- ***LABELS***: (optional) The user defined labels to organize this `PipelineJob`.  \n",
    "  Vertex AI Pipelines automatically attaches the following label to a pipeline run:  \n",
    "  `vertex-ai-pipelines-run-billing-id: pipeline_run_id`  \n",
    "  where `pipeline_run_id` is the unique ID of the pipeline run.  \n",
    "  This label connects the usage of Google Cloud resources generated by the pipeline run in billing reports.\n",
    "- ***CREDENTIALS***: (optional) Custom credentials to use to create this `PipelineJob`. Overrides credentials set in `aiplatform.init`.\n",
    "- ***PROJECT_ID***: (optional) The Google Cloud project that you want to run the pipeline in. If you don't set this parameter, the project set in `aiplatform.init` is used.\n",
    "- ***LOCATION***: (optional) The region that you want to run the pipeline in. If you don't set this parameter, the default location set in `aiplatform.init` is used.\n",
    "- ***FAILURE_POLICY***: (optional) Specify the failure policy for the entire pipeline. The following configurations are available:\n",
    "  - To configure the pipeline to fail after one task fails, enter `fast`. Tasks that are already scheduled continue running until they are completed.\n",
    "  - To configure the pipeline to continue scheduling tasks after one task fails, enter `slow`. The pipeline continues to run until all tasks have been executed.\n",
    "  If you don't set this parameter, the failure policy configuration is set to `slow`, by default.\n",
    "- ***SERVICE_ACCOUNT***: (optional) The name of the service account to use for this pipeline run. If you don't specify a service account, Vertex AI Pipelines runs your pipeline using the default Compute Engine service account.\n",
    "- ***NETWORK***: (optional) :The name of the VPC peered network to use for this pipeline run.\n",
    "\n",
    "In the output of the `job.submit()` function, you should be able to click the link that brings you to view the pipeline execution in the Google Cloud console."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d83657",
   "metadata": {},
   "source": [
    "## Configure execution caching \n",
    "When Vertex AI Pipelines runs a pipeline, it checks to see whether or not an execution exists in Vertex ML Metadata with the interface (cache key) of each pipeline step.\n",
    "\n",
    "The step's interface is defined as the combination of the following:\n",
    "- The pipeline step's inputs. These inputs include the input parameters' value (if any) and the input artifact id (if any).\n",
    "- The pipeline step's output definition. This output definition includes output parameter definition (name, if any) and output artifact definition (name, if any).\n",
    "- The component's specification. This specification includes the image, commands, arguments and environment variables being used, as well as the order of the commands and arguments.\n",
    "\n",
    "Additionally, only the pipelines with the same pipeline name will share the cache.\n",
    "\n",
    "If there is a matching execution in Vertex ML Metadata, the outputs of that execution are used and the step is skipped. This helps to reduce costs by skipping computations that were completed in a previous pipeline run.\n",
    "\n",
    "You can turn off execution caching at task level by setting the following:\n",
    "`eval_task.set_caching_options(False)`\n",
    "\n",
    "You can turn off execution caching for an entire pipeline job. When you run a pipeline using `PipelineJob()`, you can use the enable_caching argument to specify that this pipeline run does not use caching. All steps within the pipeline job will not use caching. \n",
    " \n",
    "`enable_caching=False`\n",
    "\n",
    "> Whether or not to enable caching  \n",
    "    True = enable the current run to use caching results from previous runs  \n",
    "    False = disable the current run's use of caching results from previous runs  \n",
    "    None = defer to cache option for each pipeline component in the pipeline definition  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e92062d",
   "metadata": {},
   "source": [
    "> **Important**: Pipeline components should be built to be deterministic. A given set of inputs should always produce the same output. Depending on their interface, non-deterministic pipeline components can be unexpectedly skipped due to execution caching."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8606cb6",
   "metadata": {},
   "source": [
    "The following limitations apply to this feature:\n",
    "- The cached result doesn't have a time-to-live (TTL), and can be reused as long as the entry is not deleted from the Vertex ML Metadata. If the entry is deleted from Vertex ML Metadata, the task will rerun to regenerate the result again."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d89edd6",
   "metadata": {},
   "source": [
    "## Configure retries for a pipeline task \n",
    "You can specify whether a pipeline task must be rerun if it fails, by configuring the retries for that task. You can set the number of attempts to rerun the task on failure and the delay between subsequent retries.\n",
    "\n",
    "Use the following code sample to configure the failure policy of a pipeline task named `train_op` by using the `set_retry` method in the Kubeflow Pipelines SDK:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f7651f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp import dsl\n",
    "\n",
    "@dsl.pipeline(name='custom-container-pipeline')\n",
    "def pipeline():\n",
    "  generate = generate_op()\n",
    "  train = (\n",
    "    train_op(\n",
    "      training_data=generate.outputs['training_data'],\n",
    "      test_data=generate.outputs['test_data'],\n",
    "      config_file=generate.outputs['config_file'])\n",
    "    .set_retry(\n",
    "      num_retries=NUMBER_OF_RETRIES,\n",
    "\n",
    "      # Optional, The duration of time wait after the task fails before retrying. Default is '0s'.\n",
    "      backoff_duration='BACKOFF_DURATION',\n",
    "\n",
    "      # Optional, The factor by which the backoff duration is multiplied for each subsequent retry. Default is '2.0'.\n",
    "      backoff_factor=BACKOFF_FACTOR, \n",
    "\n",
    "      # Optional, The maximum backoff duration between subsequent retries. Default maximum duration '3600s'.\n",
    "      backoff_maxk_duration='BACKOFF_MAX_DURATION'\n",
    "    )\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469a6785",
   "metadata": {},
   "source": [
    "> **Caution**: You can't pass output parameters from other pipeline tasks or pipeline input parameters as parameter values for the set_retry method. These values must be available when you compile the pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d69eea1",
   "metadata": {},
   "source": [
    "## Specify the machine configuration for a pipeline step\n",
    "By setting the machine type parameters on the pipeline step, you can manage the requirements of each step in your pipeline. If you have two training steps and one step trains on a huge data file and the second step trains on a small data file, you can allocate more memory and CPU to the first task, and fewer resources to the second task.\n",
    "\n",
    "By default, the component will run on as a Vertex AI `CustomJob` using an **e2-standard-4 machine**, with 4 core CPUs and 16GB memory.\n",
    "\n",
    "> **Note**: If you want to specify the disk space in the machine configuration, you must create a custom training job from a component by requesting Google Cloud machine resources instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea1d0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp import dsl\n",
    "\n",
    "\n",
    "@dsl.pipeline(name='custom-container-pipeline')\n",
    "def pipeline():\n",
    "  generate = generate_op()\n",
    "  train = (\n",
    "      train_op(\n",
    "          training_data=generate.outputs['training_data'],\n",
    "          test_data=generate.outputs['test_data'],\n",
    "          config_file=generate.outputs['config_file'])\n",
    "      .set_cpu_limit('CPU_LIMIT')\n",
    "      .set_memory_limit('MEMORY_LIMIT')\n",
    "      .add_node_selector_constraint(SELECTOR_CONSTRAINT)\n",
    "      .set_accelerator_limit(ACCELERATOR_LIMIT))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1073361c",
   "metadata": {},
   "source": [
    "Replace the following:\n",
    "- ***CPU_LIMIT***: The maximum CPU limit for this operator. This string value can be a number (integer value for number of CPUs), or a number followed by \"m\", which means 1/1000. You can specify at most 96 CPUs.\n",
    "- ***MEMORY_LIMIT***: The maximum memory limit for this operator. This string value can be a number, or a number followed by \"K\" (kilobyte), \"M\" (megabyte), or \"G\" (gigabyte). At most 624GB is supported.\n",
    "> **Note**: Vertex AI Pipelines does not support calling set_memory_request on an operator ; you must use set_memory_limit to request a specific memory amount.\n",
    "- ***SELECTOR_CONSTRAINT***: Each constraint is a key-value pair label. For the container to be eligible to run on a node, the node must have each constraint as a label. For example: `'cloud.google.com/gke-accelerator'`, `'NVIDIA_TESLA_T4'`  \n",
    "  Available constraints: `NVIDIA_H100_MEGA_80GB`* (includes GPUDirect-TCPXO), `NVIDIA_H100_80GB`,`NVIDIA_A100_80GB`, `NVIDIA_TESLA_A100` (NVIDIA A100 40GB), `NVIDIA_TESLA_P4`, `NVIDIA_TESLA_P100`, `NVIDIA_TESLA_T4`, `NVIDIA_TESLA_V100`, `NVIDIA_L4`, `TPU_V2`, `TPU_V3`\n",
    "- ***ACCELERATOR_LIMIT***: The accelerator (GPU or TPU) limit for the operator. You can specify a positive integer.\n",
    "\n",
    "`CustomJob` supports specific machine types that limit you to a maximum of 96 CPUs and 624GB of memory. Based on the CPU, memory, and accelerator configuration that you specify, Vertex AI Pipelines automatically selects the closest match from the supported machine types."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5594ae5d",
   "metadata": {},
   "source": [
    "## Request Google Cloud machine resources with Vertex AI Pipelines "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b36a01",
   "metadata": {},
   "source": [
    "### Create a custom training job from a component using Vertex AI Pipelines\n",
    "The following sample shows how to use the `create_custom_training_job_from_component` method to transform a Python component into a custom training job with user-defined Google Cloud machine resources, and then run the compiled pipeline on Vertex AI Pipelines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2079159d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import kfp\n",
    "from kfp import dsl\n",
    "from google_cloud_pipeline_components.v1.custom_job import create_custom_training_job_from_component\n",
    "\n",
    "# Create a Python component\n",
    "@dsl.component\n",
    "def my_python_component():\n",
    "  import time\n",
    "  time.sleep(1)\n",
    "\n",
    "# Convert the above component into a custom training job\n",
    "custom_training_job = create_custom_training_job_from_component(\n",
    "    my_python_component,\n",
    "    display_name = 'DISPLAY_NAME',\n",
    "    machine_type = 'MACHINE_TYPE',\n",
    "    accelerator_type='ACCELERATOR_TYPE',\n",
    "    accelerator_count='ACCELERATOR_COUNT',\n",
    "    boot_disk_type: 'BOOT_DISK_TYPE',\n",
    "    boot_disk_size_gb: 'BOOT_DISK_SIZE',\n",
    "    network: 'NETWORK',\n",
    "    reserved_ip_ranges: 'RESERVED_IP_RANGES',\n",
    "    nfs_mounts: 'NFS_MOUNTS'\n",
    "    persistent_resource_id: 'PERSISTENT_RESOURCE_ID'\n",
    ")\n",
    "\n",
    "# Define a pipeline that runs the custom training job\n",
    "@dsl.pipeline(\n",
    "  name=\"resource-spec-request\",\n",
    "  description=\"A simple pipeline that requests a Google Cloud machine resource\",\n",
    "  pipeline_root='PIPELINE_ROOT',\n",
    ")\n",
    "def pipeline():\n",
    "  training_job_task = custom_training_job(\n",
    "      project='PROJECT_ID',\n",
    "      location='LOCATION',\n",
    "  ).set_display_name('training-job-task')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1e092e",
   "metadata": {},
   "source": [
    "Replace the following:\n",
    "- ***DISPLAY_NAME***: The name of the custom job. If you don't specify the name, the component name is used, by default.\n",
    "- ***MACHINE_TYPE***: The type of the machine for running the custom job—for example, `e2-standard-4`. If you specified a TPU as the `accelerator_type`, set this to `cloud-tpu`.\n",
    "- ***ACCELERATOR_TYPE***: The type of accelerator attached to the machine.\n",
    "- ***ACCELERATOR_COUNT***: The number of accelerators attached to the machine running the custom job. Default is `1`.\n",
    "- ***BOOT_DISK_TYPE***: The type of boot disk.\n",
    "- ***BOOT_DISK_SIZE***: The size of the boot disk in GB.\n",
    "- ***NETWORK***: If the custom job is peered to a Compute Engine network that has private services access configured, specify the full name of the network.\n",
    "- ***RESERVED_IP_RANGES***: A list of names for the reserved IP ranges under the VPC network used to deploy the custom job.\n",
    "- ***NFS_MOUNTS***: A list of NFS mount resources in JSON dict format.\n",
    "- ***PERSISTENT_RESOURCE_ID*** (preview): The ID of the persistent resource to run the pipeline. If you specify a persistent resource, the pipeline runs on existing machines associated to the persistent resource, instead of on-demand and short-lived machine resources. Note that the network and CMEK configuration for the pipeline must match the configuration specified for the persistent resource.\n",
    "- ***PIPELINE_ROOT***: Specify a Cloud Storage URI that your pipelines service account can access. The artifacts of your pipeline runs are stored within the pipeline root."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba08b4d",
   "metadata": {},
   "source": [
    "## Configure secrets with Secret Manager\n",
    "You can use Secret Manager's Python client with Vertex AI Pipelines to access secrets stored on Secret Manager."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e4fe30",
   "metadata": {},
   "source": [
    "### Build and run a pipeline with Python function based components\n",
    "1. Grant the service account that runs the pipeline with secrete manager permission.\n",
    "2. Using Kubeflow Pipelines SDK, build a simple pipeline with one task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad763734",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp import compiler\n",
    "from kfp import dsl\n",
    "\n",
    "# A simple component that prints a secret stored in Secret Manager\n",
    "# Be sure to specify \"google-cloud-secret-manager\" as one of packages_to_install\n",
    "@dsl.component(\n",
    "    packages_to_install=['google-cloud-secret-manager']\n",
    ")\n",
    "def print_secret_op(project_id: str, secret_id: str, version_id: str) -> str:\n",
    "    from google.cloud import secretmanager\n",
    "\n",
    "    secret_client = secretmanager.SecretManagerServiceClient()\n",
    "    secret_name = f'projects/{project_id}/secrets/{secret_id}/versions/{version_id}'\n",
    "    response = secret_client.access_secret_version(request={\"name\": secret_name})\n",
    "    payload = response.payload.data.decode(\"UTF-8\")\n",
    "    answer = \"The secret is: {}\".format(payload)\n",
    "    print(answer)\n",
    "    return answer\n",
    "\n",
    "# A simple pipeline that contains a single print_secret task\n",
    "@dsl.pipeline(\n",
    "    name='secret-manager-demo-pipeline')\n",
    "def secret_manager_demo_pipeline(project_id: str, secret_id: str, version_id: str):\n",
    "    print_secret_task = print_secret_op(project_id, secret_id, version_id)\n",
    "\n",
    "# Compile the pipeline\n",
    "compiler.Compiler().compile(pipeline_func=secret_manager_demo_pipeline,\n",
    "                            package_path='secret_manager_demo_pipeline.yaml')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123fe3f4",
   "metadata": {},
   "source": [
    "3. Run the pipeline using the Vertex AI SDK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8234ae40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "\n",
    "parameter_values = {\n",
    "    \"project_id\": PROJECT_ID,\n",
    "    \"secret_id\": SECRET_ID,     # Id (name) of secret\n",
    "    \"version_id\": VERSION_ID    # The version name of the secret\n",
    "}\n",
    "\n",
    "aiplatform.init(\n",
    "    project=PROJECT_ID,\n",
    "    location=REGION,\n",
    ")\n",
    "\n",
    "job = aiplatform.PipelineJob(\n",
    "    display_name=f'test-secret-manager-pipeline',\n",
    "    template_path='secret_manager_demo_pipeline.yaml',\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    "    enable_caching=False,\n",
    "    parameter_values=parameter_values\n",
    ")\n",
    "\n",
    "job.submit(\n",
    "    service_account=SERVICE_ACCOUNT\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb35edd3",
   "metadata": {},
   "source": [
    "## Output HTML and Markdown\n",
    "Vertex AI Pipelines provides a set of pre-defined visualization types for evaluating the result of a pipeline job (for example, Metrics, ClassificationMetrics). However, there are many cases where custom visualization is needed. Vertex AI Pipelines provides two main approaches to output custom visualization artifacts: Markdown and HTML files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8164210d",
   "metadata": {},
   "source": [
    "### Import required dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b67ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp import dsl\n",
    "from kfp.dsl import (\n",
    "    Output,\n",
    "    HTML,\n",
    "    Markdown\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58cc4373",
   "metadata": {},
   "source": [
    "### Output HTML\n",
    "To export an HTML file, define a component with the `Output[HTML]` artifact. You also must write HTML content to the artifact's path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f3b5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component\n",
    "def html_visualization(html_artifact: Output[HTML]):\n",
    "    public_url = 'https://user-images.githubusercontent.com/37026441/140434086-d9e1099b-82c7-4df8-ae25-83fda2929088.png'\n",
    "    html_content = \\\n",
    "      '<html><head></head><body><h1>Global Feature Importance</h1>\\n<img src=\"{}\" width=\"97%\"/></body></html>'.format(public_url)\n",
    "    with open(html_artifact.path, 'w') as f:\n",
    "        f.write(html_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63e5775",
   "metadata": {},
   "source": [
    "### Output Markdown\n",
    "To export a Markdown file, define a component with the `Output[Markdown]` artifact. You also must write Markdown content to the artifact's path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad45f0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component\n",
    "def markdown_visualization(markdown_artifact: Output[Markdown]):\n",
    "    import urllib.request\n",
    "\n",
    "    with urllib.request.urlopen('https://gist.githubusercontent.com/zijianjoy/a288d582e477f8021a1fcffcfd9a1803/raw/68519f72abb59152d92cf891b4719cd95c40e4b6/table_visualization.md') as table:\n",
    "        markdown_content = table.read().decode('utf-8')\n",
    "        with open(markdown_artifact.path, 'w') as f:\n",
    "            f.write(markdown_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e217124c",
   "metadata": {},
   "source": [
    "### Create your pipeline\n",
    "After you have defined your component with the HTML or Markdown artifact create and run a pipeline that use the component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9acf6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "    name=f'metrics-visualization-pipeline')\n",
    "def metrics_visualization_pipeline():\n",
    "    html_visualization_op = html_visualization()\n",
    "    markdown_visualization_op = markdown_visualization()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
