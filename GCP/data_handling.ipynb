{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c53795ef",
   "metadata": {},
   "source": [
    "## Data Types\n",
    "KFP components and pipelines can accept inputs and create outputs. To do so, they must declare typed interfaces through their function signatures and annotations.\n",
    "\n",
    "There are two groups of types in KFP: parameters and artifacts. Parameters are useful for passing small amounts of data between components. Artifacts types are the mechanism by which KFP provides first-class support for ML artifact outputs, such as datasets, models, metrics, etc.\n",
    "\n",
    "KFP automatically tracks the way parameters and artifacts are passed between components and stores the this data passing history in ML Metadata. This enables out-of-the-box ML artifact lineage tracking and easily reproducible pipeline executions. Furthermore, KFP’s strongly-typed components provide a data contract between tasks in a pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897fa7dc",
   "metadata": {},
   "source": [
    "## Pass small amounts of data between components\n",
    "Parameters are useful for passing small amounts of data between components and when the data created by a component does not represent a machine learning artifact such as a model, dataset, or more complex data type. As with normal Python function, input parameters can have default values.\n",
    "\n",
    "KFP maps Python type annotations to the types stored in ML Metadata according to the following table:\n",
    "\n",
    "Python object   | KFP type\n",
    "----------------|---------\n",
    "str\t            | string\n",
    "int\t            | number\n",
    "float\t        | number\n",
    "bool\t        | boolean\n",
    "typing.List / list\t| object\n",
    "typing.Dict / dict\t| object"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf121d2",
   "metadata": {},
   "source": [
    "Under the hood KFP passes all parameters to and from components by serializing them as JSON. For all Python Component, parameter serialization and deserialization is invisible to the user; KFP handles this automatically. For Container Components, input parameter deserialization is invisible to the user; KFP passes inputs to the component automatically. For Container Component outputs, the user code in the Container Component must handle serializing the output parameters.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930400a7",
   "metadata": {},
   "source": [
    "### Input parameters\n",
    "Using input parameters is very easy. Simply annotate your component function with the types and, optionally, defaults."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f0ff8a",
   "metadata": {},
   "source": [
    "### Output parameters\n",
    "For Python Components and pipelines, output parameters are indicated via return annotations.\n",
    "\n",
    "For Container Components, output parameters are indicated using a `dsl.OutputPath` annotation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c41aec9",
   "metadata": {},
   "source": [
    "### Multiple output parameters \n",
    "You can specify multiple named output parameters using a `typing.NamedTuple`. You can access a named output using `.outputs['<output-key>']` on `PipelineTask`:\n",
    "\n",
    "```python\n",
    "def my_comp() -> NamedTuple('outputs', a=int, b=str)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74fe1dd",
   "metadata": {},
   "source": [
    "## Create, use, pass, and track ML artifacts\n",
    "KFP provides first-class support for creating machine learning artifacts via the `dsl.Artifact` class and other artifact subclasses. KFP maps these artifacts to their underlying ML Metadata schema title, the canonical name for the artifact type.  \n",
    "Artifacts are simply a thin wrapper around some artifact properties, including the `.path` from which the artifact can be read/written and the artifact’s `.metadata`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669ecd9b",
   "metadata": {},
   "source": [
    "### Artifact properties\n",
    "To use create and consume artifacts from components, you’ll use the available properties on artifact instances. Artifacts feature four properties:\n",
    "- `name`, the name of the artifact (cannot be overwritten on Vertex Pipelines).\n",
    "- `.uri`, the location of your artifact object. For input artifacts, this is where the object resides currently. For output artifacts, this is where you will write the artifact from within your component.\n",
    "- `.metadata`, additional key-value pairs about the artifact.\n",
    "- `.path`, a local path that corresponds to the artifact’s `.uri`.\n",
    "\n",
    "The artifact `.path` attribute is particularly helpful. When you write the contents of your artifact to the location provided by the artifact’s `.path` attribute, the pipelines backend will handle copying the file at `.path` to the URI at `.uri` automatically, allowing you to create artifact files within a component by only interacting with the task’s local filesystem.\n",
    "\n",
    "Note that input artifacts should be treated as immutable. You should not try to modify the contents of the file at `.path` and any changes to the artifact’s properties will not affect the artifact’s metadata in ML Metadata."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac04488",
   "metadata": {},
   "source": [
    "### Artifacts in components\n",
    "The KFP SDK supports two forms of artifact authoring syntax for components: traditional and Pythonic.\n",
    "\n",
    "The **traditional artifact** authoring syntax is the original artifact authoring style provided by the KFP SDK. The traditional artifact authoring syntax is supported for both Python Components and Container Components. It is supported at runtime by the open source KFP backend and the Google Cloud Vertex Pipelines backend.\n",
    "\n",
    "The **Pythonic artifact** authoring syntax provides an alterative artifact I/O syntax that is familiar to Python developers. The Pythonic artifact authoring syntax is supported for Python Components only. This syntax is not supported for Container Components. It is currently only supported at runtime by the Google Cloud Vertex Pipelines backend."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf5d3d7",
   "metadata": {},
   "source": [
    "#### Traditional artifact syntax\n",
    "When using the traditional artifact authoring syntax, all artifacts are provided to the component function as an input wrapped in an Input or Output type marker.\n",
    "\n",
    "```python\n",
    "def my_component(in_artifact: Input[Artifact], out_artifact: Output[Artifact]):\n",
    "    ...\n",
    "```\n",
    "\n",
    "For input artifacts, you can read the artifact using its `.uri` or `.path` attribute.\n",
    "\n",
    "For output artifacts, a pre-constructed output artifact will be passed into the component. You can update the output artifact’s properties in place and write the artifact’s contents to the artifact’s `.path` or `.uri` attribute. You should not return the artifact instance from your component."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb42225c",
   "metadata": {},
   "source": [
    "#### New Pythonic artifact syntax\n",
    "To use the Pythonic artifact authoring syntax, simply annotate your components with the artifact class as you would when writing normal Python.\n",
    "\n",
    "```python\n",
    "def my_component(in_artifact: Artifact) -> Artifact:\n",
    "    ...\n",
    "```\n",
    "\n",
    "Inside the body of your component, you can read artifacts passed in as input (no change from the traditional artifact authoring syntax). For artifact outputs, you’ll construct the artifact in your component code, then return the artifact as an output.\n",
    "\n",
    "Multiple output artifacts should be specified similarly to multiple output parameters:\n",
    "\n",
    "```python\n",
    "def train_multiple_models(dataset: Dataset,) -> NamedTuple('outputs', model1=Model, model2=Model):\n",
    "    ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f573a7",
   "metadata": {},
   "source": [
    "### Artifacts in pipelines\n",
    "Irrespective of whether your components use the Pythonic or traditional artifact authoring syntax, pipelines that use artifacts should be annotated with the Pythonic artifact syntax."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "678353e9",
   "metadata": {},
   "source": [
    "### Lists of artifacts\n",
    "KFP supports input lists of artifacts, annotated as `List[Artifact]` or `Input[List[Artifact]]`. This is useful for collecting output artifacts from a loop of tasks using the `dsl.ParallelFor` and `dsl.Collected` control flow objects.\n",
    "\n",
    "Pipelines can also return an output list of artifacts by using a `-> List[Artifact]` return annotation and returning a `dsl.Collected` instance.\n",
    "\n",
    "Creating output lists of artifacts from a single-step component is not currently supported."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db552959",
   "metadata": {},
   "source": [
    "### Artifact types\n",
    "The artifact annotation indicates the type of the artifact. KFP provides several artifact types within the DSL:\n",
    "\n",
    "DSL object\t                | Artifact schema title\n",
    "----------------------------|----------------------\n",
    "`Artifact`\t                | system.Artifact\n",
    "`Dataset`\t                | system.Dataset\n",
    "`Model`\t                    | system.Model\n",
    "`Metrics`\t                | system.Metrics\n",
    "`ClassificationMetrics`\t    | system.ClassificationMetrics\n",
    "`SlicedClassificationMetrics`|\tsystem.SlicedClassificationMetrics\n",
    "`HTML`\t                    | system.HTML\n",
    "`Markdown`\t                | system.Markdown\n",
    "\n",
    "`Artifact`, `Dataset`, `Model`, and `Metrics` are the most generic and commonly used artifact types. `Artifact` is the default artifact base type and should be used in cases where the artifact type does not fit neatly into another artifact category. `Artifact` is also compatible with all other artifact types. In this sense, the `Artifact` type is also an artifact “any” type."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b25f15",
   "metadata": {},
   "source": [
    "## PipelineParameterChannel\n",
    "A **PipelineParameterChannel* in Kubeflow Pipelines represents a future value that is passed between pipeline components. It can be used as a pipeline function argument, making it a pipeline artifact or parameter that appears in the ML Pipelines system UI. Essentially, it allows for the dynamic passing of data between different parts of the pipeline.\n",
    "\n",
    "Here are some key points about PipelineParameterChannel:\n",
    "- **Future Value**: It represents a value that will be available in the future, typically produced by a task within the pipeline.\n",
    "- **Pipeline Function Argument**: It can be used as an argument in pipeline functions, enabling the passing of parameters or artifacts between components.\n",
    "- **Intermediate Value**: It can also represent intermediate values passed between tasks, allowing for complex workflows and data dependencies.\n",
    "\n",
    "For example, when you define a pipeline function with parameters, each parameter becomes a PipelineParameterChannel object. These objects can then be passed to components as arguments, creating tasks and managing data flow within the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d56532",
   "metadata": {},
   "source": [
    "> `Output[Artifact]`: gets a metadata-rich handle to the output artifact of type `Artifact`. Use `OutputArtifact.path` to access a local file path for writing. One can also use `OutputArtifact.uri` to access the actual URI file path.\n",
    "\n",
    "> `OutputPath(\"Artifact\")`: A locally accessible filepath for another output artifact of type `Artifact`. `OutputPath` is used to just pass the local file path of the output artifact to the function.\n",
    "\n",
    "> `OutputPath(str)`: A locally accessible filepath for an output parameter of type string.\n",
    "\n",
    "> `Output[Model]`: Output artifact of type of model. Use `model.get()` to get a Model artifact, which has a `.metadata` dictionary to store arbitrary metadata for the output artifact. This metadata is recorded in Managed Metadata and can be queried later. It also shows up in the Google Cloud console."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe3c50b",
   "metadata": {},
   "source": [
    "> `InputPath(\"Dataset\")`: Use `InputPath` to get a locally accessible path for the input artifact of type `Dataset`. Directly access the passed in GCS URI as a local file (uses GCSFuse).\n",
    "\n",
    "> `Input[Dataset]`: Use `InputArtifact` to get a metadata-rich handle to the input artifact of type `Dataset`. This gives an `Artifact` handle. Use `InputArtifact.path` to get a local file path (uses GCSFuse). Alternately, use `InputArtifact.uri` to access the GCS URI directly.\n",
    "\n",
    "> Use `NamedTuple` to return either artifacts or parameters. When returning artifacts like this, return the contents of the artifact. The assumption here is that this return value fits in memory."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
