{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ec693a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "pip install --upgrade google-cloud-pipeline-components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5f11ed",
   "metadata": {},
   "source": [
    "## Google Cloud Pipeline Components\n",
    "The Google Cloud (GCPC) SDK provides a set of prebuilt Kubeflow Pipelines components that are production quality, performant, and easy to use.\n",
    "\n",
    "For example, you can use these components to complete the following:\n",
    "- Create a new dataset and load different data types into the dataset (image, tabular, text, or video).\n",
    "- Export data from a dataset to Cloud Storage.\n",
    "- Use AutoML to train a model using image, tabular, text, or video data.\n",
    "- Run a custom training job using a custom container or a Python package.\n",
    "- Upload an existing model to Vertex AI for batch prediction.\n",
    "- Create a new endpoint and deploy a model to it for online predictions.  \n",
    "\n",
    "Additionally, these prebuilt Google Cloud Pipeline Components are supported in Vertex AI Pipelines and offer the following benefits:\n",
    "- **Easier debugging**: Show the underlying resources launched from the component for simplified debugging.\n",
    "- **Standardized artifact types**: Provide consistent interfaces to use standard artifact types for input and output. These standard artifacts are tracked in Vertex ML Metadata, making it easier to analyze the lineage of your pipeline's artifacts.\n",
    "- **Understand pipeline costs with billing labels**: Resource labels are automatically propagated to Google Cloud services generated by the Google Cloud Pipeline Components in your pipeline run. You can use billing labels along with Cloud Billing export to BigQuery to review the cost of your pipeline run.\n",
    "- **Cost efficiencies***: Vertex AI Pipelines optimize the execution of these components by launching the Google Cloud resources, without having to launch the container. This reduces the startup latency and reduces the costs of the busy-waiting container.\n",
    "\n",
    ">\\*\tThis feature applies to the following components only:  \n",
    "    - CustomTrainingJobOp  \n",
    "    - WaitGcpResourcesOp (for Dataflow)  \n",
    "    - DataflowPythonJobOp  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41fe631b",
   "metadata": {},
   "source": [
    "**[Google Cloud Pipeline Components list](https://cloud.google.com/vertex-ai/docs/pipelines/gcpc-list)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486c2c18",
   "metadata": {},
   "source": [
    "## Use Google Cloud Pipeline Components \n",
    "When you use Google Cloud Pipeline Components (GCPC), you can use the following Vertex AI and Google Cloud features to secure your components and artifacts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36bd4ff",
   "metadata": {},
   "source": [
    "### Specify a service account for a component\n",
    "When you use a component, you can optionally specify a service account. Your component launches and acts with the permissions of this service account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e5eeec",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_deploy_op = ModelDeployOp(model=training_job_run_op.outputs[\"model\"],\n",
    "    endpoint=endpoint_op.outputs[\"endpoint\"],\n",
    "    automatic_resources_min_replica_count=1,\n",
    "    automatic_resources_max_replica_count=1,\n",
    "    service_account=\"SERVICE_ACCOUNT_ID@PROJECT_ID.iam.gserviceaccount.com\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efef66cd",
   "metadata": {},
   "source": [
    "### Use VPC Service Controls to prevent data exfiltration\n",
    "VPC Service Controls can help you mitigate the risk of data exfiltration from Vertex AI Pipelines. When you use VPC Service Controls to create a service perimeter, resources and data that are created by Vertex AI Pipelines and the Google Cloud Pipeline Components are automatically protected. For example, when you use VPC Service Controls to protect your pipeline, the following artifacts can't leave your service perimeter:\n",
    "- Training data for an AutoML model\n",
    "- Models that you created\n",
    "- Results from a batch prediction request"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88ad05b",
   "metadata": {},
   "source": [
    "### Set up VPC Network Peering\n",
    "You can configure Google Cloud Pipeline Components to peer with a Virtual Private Cloud by providing extra parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5a4716",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_create_op = EndpointCreateOp(\n",
    "    project=\"PROJECT_ID\",\n",
    "    location=\"REGION\",\n",
    "    display_name=\"endpoint-display-name\",\n",
    "    network=\"NETWORK\")      # The VPC network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721c47ce",
   "metadata": {},
   "source": [
    "### Use customer-managed encryption keys (CMEK)\n",
    "By default, Google Cloud automatically encrypts data when at rest using encryption keys managed by Google. If you have specific compliance or regulatory requirements related to the keys that protect your data, you can use customer-managed encryption keys (CMEK) for your resources."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce65f78c",
   "metadata": {},
   "source": [
    "#### Configuring your component with CMEK\n",
    "After you create a key ring and key in Cloud Key Management Service, and grant Vertex AI encrypter and decrypter permissions for your key, you can create a new CMEK-supported component by specifying your key as one of the create parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90fe1222",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_batch_predict_op = ModelBatchPredictOp(project=\"PROJECT_ID\",\n",
    "    model=model_upload_op.outputs[\"model\"],\n",
    "    encryption_spec_key_name=\"projects/PROJECT_ID/locations/LOCATION_ID/keyRings/KEY_RING_NAME/cryptoKeys/KEY_NAME\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f06fac",
   "metadata": {},
   "source": [
    "> **Note**: Google Cloud components that aren't Vertex AI components might require additional permissions. For example, a BigQuery component might require encryption and decryption permission. In addition, the location of the CMEK key must be the same as the location of the component."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9451202c",
   "metadata": {},
   "source": [
    "### Consume or produce artifacts in your component\n",
    "The Google Cloud SDK defines a set of ML metadata artifact types that serve as component input and output. Some Google Cloud Pipeline Components consume these artifacts as input or produce them as output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c94e5124",
   "metadata": {},
   "source": [
    "#### Consume an artifact in component YAML\n",
    "The artifact's metadata can serve as input to a component. To prepare an artifact to be consumed as input, you must extract it and put it in a component YAML file.\n",
    "\n",
    "For example, the `ModelUploadOp` component generates a `google.VertexModel` artifact which can be consumed by a `ModelDeployOp` component. Use the following code in a component YAML file to retrieve the a Vertex AI Model resource from the inputs (reference):  \n",
    "`\"model\": \"',\"{{$.inputs.artifacts['model'].metadata['resourceName']}}\", '\"'`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b208ac3",
   "metadata": {},
   "source": [
    "#### Consume an artifact in a lightweight Python component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70cf4f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp.dsl import Artifact, Input\n",
    "\n",
    "@dsl.component\n",
    "def classification_model_eval_metrics(\n",
    "    project: str,\n",
    "    location: str,  # \"us-central1\",\n",
    "    model: Input[Artifact],\n",
    ") :\n",
    "   # Consumes the `resourceName` metadata\n",
    "   model_resource_path = model.metadata[\"resourceName\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cbdcb93",
   "metadata": {},
   "source": [
    "### Create an ML artifact"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70763b66",
   "metadata": {},
   "source": [
    "#### Use an importer node\n",
    "The following example creates an Importer node that registers a new artifact entry to Vertex ML Metadata. The importer node takes the artifact's URI and metadata as primitives and packages them into an artifact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1a05de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google_cloud_pipeline_components import v1\n",
    "from google_cloud_pipeline_components.types import artifact_types\n",
    "from kfp.components import importer_node\n",
    "from kfp import dsl\n",
    "\n",
    "@dsl.pipeline(name=_PIPELINE_NAME)\n",
    "def pipeline():\n",
    "  # Using importer and UnmanagedContainerModel artifact for model upload\n",
    "  # component.\n",
    "  importer_spec = importer_node.importer(\n",
    "      artifact_uri='gs://managed-pipeline-gcpc-e2e-test/automl-tabular/model',\n",
    "      artifact_class=artifact_types.UnmanagedContainerModel,\n",
    "      metadata={\n",
    "          'containerSpec': {\n",
    "              'imageUri':\n",
    "                  'us-docker.pkg.dev/vertex-ai/automl-tabular/prediction-server:prod'\n",
    "          }\n",
    "      })\n",
    "\n",
    "  # Consuming the UnmanagedContainerModel artifact for the previous step\n",
    "  model_upload_with_artifact_op = v1.model.ModelUploadOp(\n",
    "      project=_GCP_PROJECT_ID,\n",
    "      location=_GCP_REGION,\n",
    "      display_name=_MODEL_DISPLAY_NAME,\n",
    "      unmanaged_container_model=importer_spec.outputs['artifact'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0521b2e9",
   "metadata": {},
   "source": [
    "#### Use Python function-based components\n",
    "The following example shows how to output a Vertex ML Metadata artifact directly from a Python component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a92d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google_cloud_pipeline_components import v1\n",
    "from kfp.components import importer_node\n",
    "from kfp import dsl\n",
    "\n",
    "@dsl.component(\n",
    "    base_image='python:3.9',\n",
    "    packages_to_install=['google-cloud-aiplatform'],\n",
    ")\n",
    "# Note currently KFP SDK doesn't support outputting artifacts in `google` namespace.\n",
    "# Use the base type dsl.Artifact instead.\n",
    "def return_unmanaged_model(model: dsl.Output[dsl.Artifact]):\n",
    "  model.metadata['containerSpec'] = {\n",
    "      'imageUri':\n",
    "          'us-docker.pkg.dev/vertex-ai/automl-tabular/prediction-server:prod'\n",
    "  }\n",
    "  model.uri = f'gs://automl-tabular-pipeline/automl-tabular/model'\n",
    "\n",
    "@dsl.pipeline(name=_PIPELINE_NAME)\n",
    "def pipeline():\n",
    "\n",
    "  unmanaged_model_op = return_unmanaged_model()\n",
    "\n",
    "  # Consuming the UnmanagedContainerModel artifact for the previous step\n",
    "  model_upload_with_artifact_op = v1.model.ModelUploadOp(\n",
    "      project=_GCP_PROJECT_ID,\n",
    "      location=_GCP_REGION,\n",
    "      display_name=_MODEL_DISPLAY_NAME,\n",
    "      unmanaged_container_model=unmanaged_model_op.outputs['model'])\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a4e4d37",
   "metadata": {},
   "source": [
    "#### Use your own container-based component\n",
    "The following example shows how to generate a `VertexBatchPredictionJob` artifact as output from a container-based component using the [artifact_types.py](https://github.com/kubeflow/pipelines/blob/master/components/google-cloud/google_cloud_pipeline_components/types/artifact_types.py) utility class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1f1f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "bp_job_artifact = VertexBatchPredictionJob(\n",
    "    'batchpredictionjob', vertex_uri_prefix + get_job_response.name,\n",
    "    get_job_response.name, get_job_response.output_info.bigquery_output_table,\n",
    "    get_job_response.output_info.bigquery_output_dataset,\n",
    "    get_job_response.output_info.gcs_output_directory)\n",
    "\n",
    "    output_artifacts = executor_input_json.get('outputs', {}).get('artifacts', {})\n",
    "    executor_output['artifacts'] = bp_job_artifact.to_executor_output_artifact(output_artifacts[bp_job_artifact.name])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d55282f",
   "metadata": {},
   "source": [
    "## Build your own pipeline components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aac0292",
   "metadata": {},
   "source": [
    "### Write a component to show a Google Cloud console link\n",
    "It's common that when running a component, you want to not only see the link to the component job being launched, but also the link to the underlying cloud resources, such as the Vertex batch prediction jobs or dataflow jobs.\n",
    "\n",
    "The [`gcp_resource` proto](https://github.com/kubeflow/pipelines/tree/master/components/google-cloud/google_cloud_pipeline_components/proto) is a special parameter that you can use in your component to enable the Google Cloud console to provide a customized view of the resource's logs and status in the Vertex AI Pipelines console."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154d3efb",
   "metadata": {},
   "source": [
    "#### Output the gcp_resource parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0facff09",
   "metadata": {},
   "source": [
    "##### Using a container-based component\n",
    "First, you'll need to define the gcp_resource parameter in your component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a822ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "from google_cloud_pipeline_components import _image\n",
    "from google_cloud_pipeline_components import _placeholders\n",
    "from kfp.dsl import container_component\n",
    "from kfp.dsl import ContainerSpec\n",
    "from kfp.dsl import OutputPath\n",
    "\n",
    "\n",
    "@container_component\n",
    "def dataflow_python(\n",
    "    python_module_path: str,\n",
    "    temp_location: str,\n",
    "    gcp_resources: OutputPath(str),\n",
    "    location: str = 'us-central1',\n",
    "    requirements_file_path: str = '',\n",
    "    args: List[str] = [],\n",
    "    project: str = _placeholders.PROJECT_ID_PLACEHOLDER,\n",
    "):\n",
    "  # fmt: off\n",
    "  \"\"\"Launch a self-executing Beam Python file on Google Cloud using the\n",
    "  Dataflow Runner.\n",
    "\n",
    "  Args:\n",
    "      location: Location of the Dataflow job. If not set, defaults to `'us-central1'`.\n",
    "      python_module_path: The GCS path to the Python file to run.\n",
    "      temp_location: A GCS path for Dataflow to stage temporary job files created during the execution of the pipeline.\n",
    "      requirements_file_path: The GCS path to the pip requirements file.\n",
    "      args: The list of args to pass to the Python file. Can include additional parameters for the Dataflow Runner.\n",
    "      project: Project to create the Dataflow job. Defaults to the project in which the PipelineJob is run.\n",
    "\n",
    "  Returns:\n",
    "      gcp_resources: Serialized gcp_resources proto tracking the Dataflow job. For more details, see https://github.com/kubeflow/pipelines/blob/master/components/google-cloud/google_cloud_pipeline_components/proto/README.md.\n",
    "  \"\"\"\n",
    "  # fmt: on\n",
    "  return ContainerSpec(\n",
    "      image=_image.GCPC_IMAGE_TAG,\n",
    "      command=[\n",
    "          'python3',\n",
    "          '-u',\n",
    "          '-m',\n",
    "          'google_cloud_pipeline_components.container.v1.dataflow.dataflow_launcher',\n",
    "      ],\n",
    "      args=[\n",
    "          '--project',\n",
    "          project,\n",
    "          '--location',\n",
    "          location,\n",
    "          '--python_module_path',\n",
    "          python_module_path,\n",
    "          '--temp_location',\n",
    "          temp_location,\n",
    "          '--requirements_file_path',\n",
    "          requirements_file_path,\n",
    "          '--args',\n",
    "          args,\n",
    "          '--gcp_resources',\n",
    "          gcp_resources,\n",
    "      ],\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4576c2",
   "metadata": {},
   "source": [
    "Next, inside the container, install the Google Cloud Pipeline Components package:  \n",
    "`pip install --upgrade google-cloud-pipeline-components`   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b85ffa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google_cloud_pipeline_components.proto.gcp_resources_pb2 import GcpResources\n",
    "from google.protobuf.json_format import MessageToJson\n",
    "\n",
    "dataflow_resources = GcpResources()\n",
    "dr = dataflow_resources.resources.add()\n",
    "dr.resource_type='DataflowJob'\n",
    "dr.resource_uri='https://dataflow.googleapis.com/v1b3/projects/[your-project]/locations/us-east1/jobs/[dataflow-job-id]'\n",
    "\n",
    "with open(gcp_resources, 'w') as f:\n",
    "    f.write(MessageToJson(dataflow_resources))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a3e96d",
   "metadata": {},
   "source": [
    "##### Using a Python component\n",
    "Alternatively, you can return the gcp_resources output parameter as you would any string output parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6c20ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(\n",
    "    base_image='python:3.9',\n",
    "    packages_to_install=['google-cloud-pipeline-components==2.19.0'],\n",
    ")\n",
    "def launch_dataflow_component(project: str, location:str) -> NamedTuple(\"Outputs\",  [(\"gcp_resources\", str)]):\n",
    "  # Launch the dataflow job\n",
    "  dataflow_job_id = [dataflow-id]\n",
    "  dataflow_resources = GcpResources()\n",
    "  dr = dataflow_resources.resources.add()\n",
    "  dr.resource_type='DataflowJob'\n",
    "  dr.resource_uri=f'https://dataflow.googleapis.com/v1b3/projects/{project}/locations/{location}/jobs/{dataflow_job_id}'\n",
    "  gcp_resources=MessageToJson(dataflow_resources)\n",
    "  return gcp_resources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4605e48",
   "metadata": {},
   "source": [
    "##### Supported resource_type values\n",
    "You can set the `resource_type` to be an arbitrary string, but only the following types have links in the Google Cloud console:\n",
    "- BatchPredictionJob\n",
    "- BigQueryJob\n",
    "- CustomJob\n",
    "- DataflowJob\n",
    "- HyperparameterTuningJob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4495ab",
   "metadata": {},
   "source": [
    "### Write a component to cancel the underlying resources\n",
    "When a pipeline job is canceled, the default behavior is for the underlying Google Cloud resources to keep running. They are not canceled automatically. To change this behavior, you should attach a `SIGTERM` handler to the pipeline job. A good place to do this is just before a polling loop for a job that could run for a long time.\n",
    "\n",
    "Cancellation has been implemented on several Google Cloud Pipeline Components, including:\n",
    "- Batch prediction job\n",
    "- BigQuery ML job\n",
    "- Custom job\n",
    "- Dataproc Serverless batch job\n",
    "- Hyperparameter tuning job\n",
    "\n",
    "For more information, including sample code that shows how to attach a `SIGTERM` handler, see the following GitHub links:\n",
    "- https://github.com/kubeflow/pipelines/blob/google-cloud-pipeline-components-2.19.0/components/google-cloud/google_cloud_pipeline_components/container/utils/execution_context.py\n",
    "- https://github.com/kubeflow/pipelines/blob/google-cloud-pipeline-components-2.19.0/components/google-cloud/google_cloud_pipeline_components/container/v1/gcp_launcher/job_remote_runner.py#L124\n",
    "\n",
    "Consider the following when implementing your SIGTERM handler:\n",
    "- Cancellation propagation works only after the component has been running for a few minutes. This is typically due to background startup tasks that need to be processed before the Python signal handlers are called.\n",
    "- Some Google Cloud resources might not have cancellation implemented. For example, creating or deleting a Vertex AI Endpoint or Model could create a long-running operation that accepts a cancellation request through its REST API, but doesn't implement the cancellation operation itself."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
