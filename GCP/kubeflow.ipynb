{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78943ee8",
   "metadata": {},
   "source": [
    "**What is Kubeflow Pipelines?**  \n",
    "Kubeflow Pipelines (KFP) is a platform for building and deploying portable and scalable machine learning (ML) workflows using containers on Kubernetes-based systems.\n",
    "\n",
    "With KFP you can author components and pipelines using the KFP Python SDK, compile pipelines to an intermediate representation YAML, and submit the pipeline to run on a KFP-conformant backend such as the open source KFP backend or Google Cloud Vertex AI Pipelines."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b233fbbc",
   "metadata": {},
   "source": [
    "**What is a pipeline?**  \n",
    "A pipeline is a description of a machine learning (ML) workflow, including all of the components in the workflow and how the components relate to each other in the form of a directed acyclic graph (DAG). The pipeline configuration includes the definition of the inputs (parameters) required to run the pipeline and the inputs and outputs of each component.\n",
    "\n",
    "When you run a pipeline, the system launches one or more Kubernetes Pods corresponding to the steps (components) in your workflow (pipeline). The Pods start Docker containers, and the containers in turn start your programs. Pipelines may also feature control flow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf8e016",
   "metadata": {},
   "source": [
    "## Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85171317",
   "metadata": {},
   "source": [
    "### Pipeline Root\n",
    "[Pipeline root](https://www.kubeflow.org/docs/components/pipelines/concepts/pipeline-root/) represents the path within an object store bucket where Kubeflow Pipelines stores a pipeline’s artifacts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22c2342",
   "metadata": {},
   "source": [
    "### Component\n",
    "A pipeline component is the fundamental building block to construct a Kubeflow Pipelines pipeline. The component structure serves the purpose of packaging a functional unit of code along with its dependencies, so that it can be run as part of a workflow in a Kubernetes environement. Components can be combined in a pipeline that creates a repeatable workflow, with individual components coordinating on inputs and outputs like parameters and artifacts.\n",
    "\n",
    "A component is similar to a programming function. It is most often implemented as a wrapper to a Python function using the KFP Python SDK. However, a KFP component goes further than a simple function, with support for code dependencies, runtime environments, and distributed execution requirements.\n",
    "\n",
    "KFP components are designed to simplify constructing and running ML workflows in a Kubernetes environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defca502",
   "metadata": {},
   "source": [
    "#### What Does a Component Consist Of?\n",
    "A KFP component consist of the following key elements:\n",
    "\n",
    "1. Code  \n",
    "    - Typically a Python function, but can be other code such as a Bash command.\n",
    "2. Dependency Support  \n",
    "    - **Python libraries** - to be installed at runtime\n",
    "    - **Environment variables** - to be available in the runtime environment\n",
    "    - **Python package indices** - (for example, private PyPi servers) if needed to support installations\n",
    "    - **Cluster resources** - to support use of ConfigMaps, Secrets, PersistentVolumeClaims, and more\n",
    "    - **Runtime dependencies** - to support CPU, memory, and GPU requests and limits\n",
    "3. Base Image\n",
    "    - Defines the base container runtime environment (defaults to a generic Python base image)\n",
    "    - May include system dependencies and pre-installed Python libraries\n",
    "4. Input/Output (I/O) Specification\n",
    "    - Individual components cannot share in-memory data with each other, so they use the following concepts to support exchanging information and publishing results:\n",
    "        - **Parameters** – for small values\n",
    "        - **Artifacts** - for larger data like model files, processed datasets, and metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0aff9b",
   "metadata": {},
   "source": [
    "#### Python-Based Components\n",
    "- `@component` wrapper helps the KFP Python SDK supply the needed context for running these functions in containers as part of a KFP pipeline.\n",
    "- By default Python function run on the default base image (`kfp.dsl.component_factory._DEFAULT_BASE_IMAGE`).\n",
    "- Layers of customization can be added to a component by supplying the name of a specific `base_image`, and `packages_to_install`.\n",
    "- since the function will run inside a container (and won’t have the script context), all Python library dependencies need to be imported within the component function.\n",
    "- use KFP’s `Output[<Artifact>]` class for creating a KFP artifact type output.\n",
    "- inputs and outputs are defined as Python function parameters.\n",
    "- dependencies can often be installed at runtime, avoiding the need for custom base containers.\n",
    "- Python-based components give close access to the Python tools that ML experimenters rely on, like modules and imports, usage information, type hints, and debugging tools."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2fc6bb6",
   "metadata": {},
   "source": [
    "#### YAML-Based Components\n",
    "The KFP backend uses YAML-based definitions to specify components. While the KFP Python SDK can do this conversion automatically when a Python-based pipeline is submitted, some use-cases can benefit from the direct YAML-based component approach.\n",
    "\n",
    "A YAML-based component definition has the following parts:\n",
    "- **Metadata**: name, description, etc.\n",
    "- **Interface**: input/output specifications (name, type, description, default value, etc).\n",
    "- **Implementation**: A specification of how to run the component given a set of argument values for the component’s inputs. The implementation section also describes how to get the output values from the component once the component has finished running.\n",
    "\n",
    "YAML-based components support system commands directly. In fact, any command (or binary) that exists on the base image can be run.\n",
    "\n",
    "YAML-based components can be loaded for use in the Python SDK alongside Python-based components:\n",
    "\n",
    "```python\n",
    "from kfp.components import load_component_from_file\n",
    "\n",
    "my_comp = load_component_from_file(\"my_component.yaml\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26ad962",
   "metadata": {},
   "source": [
    "#### “Containerize” a Component\n",
    "The KFP command-line tool contains a build command to help users “containerize” a component. This can be used to create the `Dockerfile`, `runtime-dependencies.txt`, and other supporting files, or even to build the custom image and push it to a registry. In order to use this utility, the `target_image` parameter must be set in the Python-based component definition, which itself is saved in a file.\n",
    "\n",
    "```bash\n",
    "# build Dockerfile and runtime-dependencies.txt\n",
    "kfp component build --component-filepattern the_component.py --no-build-image --platform linux/amd64 .\n",
    "```\n",
    "\n",
    "Creating and maintaining custom containers can carry a significant maintenance burden. In general, a 1-to-1 relationship between components and containers is not needed or recommended, as AI/ML work is often highly iterative. A best practice is to work with a small set of base images that can support many components. If you need more control over the container build than the kfp CLI provides, consider using a container CLI like docker or podman."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d75b97",
   "metadata": {},
   "source": [
    "### Run and Recurring Run\n",
    "A **run** is a single execution of a pipeline. Runs comprise an immutable log of all experiments that you attempt, and are designed to be self-contained to allow for reproducibility. \n",
    "\n",
    "A **recurring run**, or job in the Kubeflow Pipelines backend APIs, is a repeatable run of a pipeline. The configuration for a recurring run includes a copy of a pipeline with all parameter values specified and a run trigger."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a40156",
   "metadata": {},
   "source": [
    "### Run Trigger\n",
    "A run trigger is a flag that tells the system when a recurring run configuration spawns a new run. The following types of run trigger are available:\n",
    "\n",
    "- Periodic: for an interval-based scheduling of runs (for example: every 2 hours or every 45 minutes).\n",
    "- Cron: for specifying `cron` semantics for scheduling runs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac7c9b7",
   "metadata": {},
   "source": [
    "### Step\n",
    "A step is an execution of one of the components in the pipeline. The relationship between a step and its component is one of instantiation, much like the relationship between a run and its pipeline. In a complex pipeline, components can execute multiple times in loops, or conditionally after resolving an if/else like clause in the pipeline code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80dfb6f8",
   "metadata": {},
   "source": [
    "### Output Artifact\n",
    "An output artifact is an output emitted by a pipeline component. It’s useful for pipeline components to include artifacts so that you can provide for performance evaluation, quick decision making for the run, or comparison across different runs. Artifacts also make it possible to understand how the pipeline’s various components work. An artifact can range from a plain textual view of the data to rich interactive visualizations.\n",
    "\n",
    "Within the scope of a component, artifacts can be read (for inputs) and written (for outputs) via the `.path` attribute. The KFP backend ensures that input artifact files are copied to the executing pod’s local file system from the remote storage at runtime, so that the component function can read input artifacts from the local file system. By comparison, output artifact files are copied from the local file system of the pod to remote storage, when the component finishes running. This way, the output artifacts persist outside the pod. In both cases, the component author needs to interact with the local file system only to create persistent artifacts.\n",
    "\n",
    "```python\n",
    "@dsl.component(packages_to_install=['pandas==1.3.5'])\n",
    "def create_dataset(iris_dataset: Output[Dataset]):\n",
    "    import pandas as pd\n",
    "\n",
    "    csv_url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data'\n",
    "    col_names = [\n",
    "        'Sepal_Length', 'Sepal_Width', 'Petal_Length', 'Petal_Width', 'Labels'\n",
    "    ]\n",
    "    df = pd.read_csv(csv_url, names=col_names)\n",
    "\n",
    "    with open(iris_dataset.path, 'w') as f:\n",
    "        df.to_csv(f)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea5937b",
   "metadata": {},
   "source": [
    "### ML Metadata\n",
    "Kubeflow Pipelines backend stores runtime information of a pipeline run in Metadata store. Runtime information includes the status of a task, availability of artifacts, custom properties associated with Execution or Artifact, etc.\n",
    "\n",
    "You can view the connection between Artifacts and Executions across Pipeline Runs, if one Artifact is being used by multiple Executions in different Runs. This connection visualization is called a **Lineage Graph**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849f3ed7",
   "metadata": {},
   "source": [
    "## Core Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac434363",
   "metadata": {},
   "source": [
    "### Compile a Pipeline\n",
    "compiler creates a file called pipeline.yaml, which contains a hermetic representation of your pipeline. The output is called an `Intermediate Representation (IR) YAML`, which is a serialized `PipelineSpec` protocol buffer message.\n",
    "\n",
    "Because components are actually pipelines, you may also compile them to IR YAML."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e180200c",
   "metadata": {},
   "source": [
    "#### Type checking\n",
    "By default, the DSL compiler statically type checks a pipeline to ensure type consistency between components that pass data between one another. Static type checking helps identify component I/O inconsistencies without having to run the pipeline, shortening development iterations.\n",
    "\n",
    "Specifically, the type checker checks for type equality between the type of data a component input expects and the type of the data provided.\n",
    "\n",
    "For example, for parameters, a list input may only be passed to parameters with a `typing.List` annotation. Similarly, a float may only be passed to parameters with a `float` annotation.\n",
    "\n",
    "Input data types and annotations must also match for artifacts, with one exception: the `Artifact` type is compatible with all other artifact types. In this sense, the `Artifact` type is both the default artifact type and an artifact “any” type."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c922d09f",
   "metadata": {},
   "source": [
    "#### Compiler arguments\n",
    "The Compiler.compile method accepts the following arguments:\n",
    "\n",
    "Name\t            | Type\t        | Description\n",
    "--------------------|---------------|--------------------------------\n",
    "`pipeline_func`\t    | `function`\t    | (Required) Pipeline function constructed with the `@dsl.pipeline` or component constructed with the `@dsl.component` decorator.\n",
    "`package_path`\t    | `string`\t    | (Required) Output YAML file path. For example, `~/my_pipeline.yaml` or `~/my_component.yaml`.\n",
    "`pipeline_name`\t    | `string`\t    | (Optional) If specified, sets the name of the pipeline template in the `pipelineInfo.name` field in the compiled IR YAML output. Overrides the `name` of the pipeline or component specified by the name parameter in the `@dsl.pipeline` decorator.\n",
    "`pipeline_parameters`\t| `Dict[str, Any]`| (Optional) Map of parameter names to argument values. This lets you provide default values for pipeline or component parameters. You can override these default values during pipeline submission.\n",
    "`type_check`\t        | `bool`\t        |(Optional) Indicates whether static type checking is enabled during compilation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8428348b",
   "metadata": {},
   "source": [
    "#### IR YAML\n",
    "The IR YAML is an intermediate representation of a compiled pipeline or component. It is an instance of the `PipelineSpec` protocol buffer message type, which is a platform-agnostic pipeline representation protocol. It is considered an intermediate representation because the KFP backend compiles `PipelineSpec` to `Argo Workflow` YAML as the final pipeline definition for execution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b42adb",
   "metadata": {},
   "source": [
    "### Control Flow\n",
    "The core types of control flow in KFP pipelines are:\n",
    "1. Conditions\n",
    "2. Loops\n",
    "3. Exit handling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d1d05a",
   "metadata": {},
   "source": [
    "#### dsl.If / dsl.Elif / dsl.Else\n",
    "The `dsl.If` context manager enables conditional execution of tasks within its scope based on the output of an upstream task or pipeline input parameter.\n",
    "\n",
    "The context manager takes two arguments: a required `condition` and an optional `name`. The `condition` is a comparative expression where at least one of the two operands is an output from an upstream task or a pipeline input parameter.\n",
    "\n",
    "You may also use `dsl.Elif` and `dsl.Else` context managers immediately downstream of `dsl.If` for additional conditional control flow functionality. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25dd78d0",
   "metadata": {},
   "source": [
    "#### dsl.OneOf (unsupported)\n",
    "`dsl.OneOf` can be used to gather outputs from mutually exclusive branches into a single task output which can be consumed by a downstream task or outputted from a pipeline. Branches are mutually exclusive if exactly one will be executed. To enforce this, the KFP SDK compiler requires `dsl.OneOf` consume from tasks within a logically associated group of conditional branches and that one of the branches is a `dsl.Else` branch.\n",
    "\n",
    "You should provide task outputs to the `dsl.OneOf` using `.output` or `.outputs[<key>]`, just as you would pass an output to a downstream task. The outputs provided to `dsl.OneOf` must be of the same type and cannot be other instances of `dsl.OneOf` or `dsl.Collected`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd26da69",
   "metadata": {},
   "source": [
    "```python\n",
    "@dsl.pipeline\n",
    "def my_pipeline() -> str:\n",
    "    coin_flip_task = flip_three_sided_coin()\n",
    "    with dsl.If(coin_flip_task.output == 'heads'):\n",
    "        t1 = print_and_return(text='Got heads!')\n",
    "    with dsl.Elif(coin_flip_task.output == 'tails'):\n",
    "        t2 = print_and_return(text='Got tails!')\n",
    "    with dsl.Else():\n",
    "        t3 = print_and_return(text='Draw!')\n",
    "\n",
    "    oneof = dsl.OneOf(t1.output, t2.output, t3.output)\n",
    "    announce_result(oneof)\n",
    "    return oneof\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adac321b",
   "metadata": {},
   "source": [
    "### Loops\n",
    "Kubeflow Pipelines supports loops which cause fan-out and fan-in of tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c9427c",
   "metadata": {},
   "source": [
    "#### dsl.ParallelFor (unsupported)\n",
    "The `dsl.ParallelFor` context manager allows parallel execution of tasks over a static set of items.\n",
    "\n",
    "The context manager takes three arguments:\n",
    "- `items`: static set of items to loop over\n",
    "- `name` (optional): is the name of the loop context\n",
    "- `parallelism` (optional): is the maximum number of concurrent iterations while executing the `dsl.ParallelFor` group\n",
    "    - note, `parallelism=0` indicates unconstrained parallelism"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d98f378",
   "metadata": {},
   "source": [
    "#### dsl.Collected (unsupported)\n",
    "Use `dsl.Collected` with `dsl.ParallelFor` to gather outputs from a parallel loop of tasks. Downstream tasks might consume `dsl.Collected` outputs via an input annotated with a `List` of parameters or a `List` of artifacts.\n",
    "\n",
    "You can use `dsl.Collected` to collect outputs from nested loops in a nested list of parameters.\n",
    "For example, output parameters from two nested `dsl.ParallelFor` groups are collected in a multilevel nested list of parameters, where each nested list contains the output parameters from one of the `dsl.ParallelFor` groups. The number of nested levels is based on the number of nested `dsl.ParallelFor` contexts.\n",
    "\n",
    "By comparison, artifacts created in nested loops are collected in a flat list."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c88231e",
   "metadata": {},
   "source": [
    "```python\n",
    "@dsl.pipeline\n",
    "def my_pipeline():\n",
    "    \n",
    "    # Train a model for 1, 5, 10, and 25 epochs\n",
    "    with dsl.ParallelFor(\n",
    "        items=[1, 5, 10, 25],\n",
    "    ) as epochs:\n",
    "        train_model_task = train_model(epochs=epochs)\n",
    "        \n",
    "    # Find the model with the highest accuracy\n",
    "    max_accuracy(\n",
    "        models=dsl.Collected(train_model_task.outputs['model'])\n",
    "    )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a887905",
   "metadata": {},
   "source": [
    "### Exit handling\n",
    "Kubeflow Pipelines supports exit handlers for implementing cleanup and error handling tasks that run after the main pipeline tasks finish execution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ba5d2f",
   "metadata": {},
   "source": [
    "#### dsl.ExitHandler \n",
    "The `dsl.ExitHandler` context manager allows pipeline authors to specify an exit task which will run after the tasks within the context manager’s scope finish execution, even if one of those tasks fails.\n",
    "\n",
    "This construct is analogous to using a `try:` block followed by a `finally:` block in normal Python, where the exit task is in the `finally: block`. The context manager takes two arguments: a required `exit_task` and an optional `name`. `exit_task` accepts an instantiated `PipelineTask`.\n",
    "\n",
    "The most common use case for `dsl.ExitHandler` is to run a cleanup task after the main pipeline tasks finish execution.\n",
    "\n",
    "The task you use as an exit task may use a special input that provides access to pipeline and task status metadata, including pipeline failure or success status.\n",
    "\n",
    "You can use this special input by annotating your exit task with the `dsl.PipelineTaskFinalStatus` annotation. The argument for this parameter will be provided by the backend automatically at runtime. You should not provide any input to this annotation when you instantiate your exit task.\n",
    "\n",
    "The `.ignore_upstream_failure()` task method on `PipelineTask` enables another approach to author pipelines with exit handling behavior. Calling this method on a task causes the task to ignore failures of any specified upstream tasks (as established by data exchange or by use of `.after()`). If the task has no upstream tasks, this method has no effect."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0e3b9c",
   "metadata": {},
   "source": [
    "```python\n",
    "@dsl.pipeline\n",
    "def my_pipeline():\n",
    "    clean_up_task = clean_up_resources()\n",
    "    with dsl.ExitHandler(exit_task=clean_up_task):\n",
    "        dataset_task = create_datasets()\n",
    "        train_task = train_and_save_models(dataset=dataset_task.output)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d998e596",
   "metadata": {},
   "source": [
    "### Caching\n",
    "Caching in KFP is a feature that allows you to cache the results of a component execution and reuse them in subsequent runs. When caching is enabled for a component, KFP will reuse the component’s outputs if the component is executed again with the same inputs and parameters (and the output is still available).\n",
    "\n",
    "Caching is particularly useful when you have components that take a long time to execute or when you have components that are executed multiple times with the same inputs and parameters.\n",
    "\n",
    "If a task’s results are retrieved from cache, its representation in the UI will be marked with a green “arrow from cloud” icon.\n",
    "\n",
    "Caching is enabled by default for all components in KFP. You can disable caching for a component by calling `.set_caching_options(enable_caching=False)` on a task object.\n",
    "\n",
    "```python\n",
    "def hello_pipeline(recipient: str = 'World!') -> str:\n",
    "    hello_task = say_hello(name=recipient)\n",
    "    hello_task.set_caching_options(False)\n",
    "```\n",
    "\n",
    "You can also enable or disable caching for all components in a pipeline by setting the argument caching when submitting a pipeline for execution. This will override the caching settings for all components in the pipeline.\n",
    "\n",
    "```python\n",
    "client.create_run_from_pipeline_func(\n",
    "    hello_pipeline,\n",
    "    enable_caching=True,  # overrides the above disabling of caching\n",
    ")\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
