{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "efe08cca",
   "metadata": {},
   "source": [
    "## Schedule a pipeline run with scheduler API\n",
    "You can schedule one-time or recurring pipeline runs in Vertex AI using the scheduler API. This lets you implement continuous training in your project.\n",
    "\n",
    "After you create a schedule, it can have one of the following states:\n",
    "- `ACTIVE`: An active schedule continuously creates pipeline runs according to the frequency configured using the cron schedule expression. A schedule becomes active on its start time and remains in that state until the specified end time, or until you pause it.\n",
    "- `PAUSED`: A paused schedule doesn't create pipeline runs. You can resume a paused schedule to make it active again. When you resume a paused schedule, you can use the catch_up parameter to specify whether skipped runs (runs that would have been scheduled if the schedule had been active) need to be rescheduled and submitted at the earliest possible schedule.\n",
    "- `COMPLETED`: A completed schedule no longer creates new pipeline runs. A schedule is completed according to its specified end time.\n",
    "\n",
    "Before you schedule a pipeline run using the scheduler API, use the following instructions to set up your Google Cloud project and development environment in the Google Cloud console.\n",
    "1. Grant the at least one of the following IAM permissions to the user or service account for using the scheduler API:\n",
    "    - `roles/aiplatform.admin`\n",
    "    - `roles/aiplatform.user`\n",
    "2. Build and compile a pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e5f716",
   "metadata": {},
   "source": [
    "### Create a schedule\n",
    "You can create a one-time or recurring schedule.\n",
    "- Create a schedule based on a `PipelineJob` using the `PipelineJob.create_schedule` method.\n",
    "- Creating a schedule using the `PipelineJobSchedule.create` method.\n",
    "\n",
    "While creating a pipeline run schedule, you can also pass the following placeholders supported by the KFP SDK as inputs:\n",
    "- `{{$.pipeline_job_name_placeholder}}`\n",
    "- `{{$.pipeline_job_resource_name_placeholder}}`\n",
    "- `{{$.pipeline_job_id_placeholder}}`\n",
    "- `{{$.pipeline_task_name_placeholder}}`\n",
    "- `{{$.pipeline_task_id_placeholder}}`\n",
    "- `{{$.pipeline_job_create_time_utc_placeholder}}`\n",
    "- `{{$.pipeline_job_schedule_time_utc_placeholder}}`\n",
    "- `{{$.pipeline_root_placeholder}}`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a121bb",
   "metadata": {},
   "source": [
    "#### Create a schedule from a PipelineJob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb5f74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "\n",
    "pipeline_job = aiplatform.PipelineJob(\n",
    "  template_path=\"COMPILED_PIPELINE_PATH\",\n",
    "  pipeline_root=\"PIPELINE_ROOT_PATH\",\n",
    "  display_name=\"DISPLAY_NAME\",\n",
    ")\n",
    "\n",
    "pipeline_job_schedule = pipeline_job.create_schedule(\n",
    "    # The name of the pipeline schedule. You can specify a name having a maximum length of 128 UTF-8 characters.\n",
    "    display_name=\"SCHEDULE_NAME\",\n",
    "    cron=\"TZ=CRON\",  # Cron schedule expression representing the frequency to schedule and execute pipeline runs\n",
    "    # The maximum number of concurrent runs for the schedule.\n",
    "    max_concurrent_run_count=MAX_CONCURRENT_RUN_COUNT,\n",
    "    # The maximum number of pipeline runs that the schedule creates after which it's completed.\n",
    "    max_run_count=MAX_RUN_COUNT,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa5c630",
   "metadata": {},
   "source": [
    "#### Create a schedule using PipelineJobSchedule.create"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f770f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "\n",
    "pipeline_job = aiplatform.PipelineJob(\n",
    "    template_path=\"COMPILED_PIPELINE_PATH\",\n",
    "    pipeline_root=\"PIPELINE_ROOT_PATH\",\n",
    "    display_name=\"DISPLAY_NAME\",\n",
    ")\n",
    "\n",
    "pipeline_job_schedule = aiplatform.PipelineJobSchedule(\n",
    "    pipeline_job=pipeline_job,\n",
    "    display_name=\"SCHEDULE_NAME\"\n",
    ")\n",
    "\n",
    "pipeline_job_schedule.create(\n",
    "    cron=\"TZ=CRON\",\n",
    "    max_concurrent_run_count=MAX_CONCURRENT_RUN_COUNT,\n",
    "    max_run_count=MAX_RUN_COUNT,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da55ad8",
   "metadata": {},
   "source": [
    "### List schedules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed871673",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "\n",
    "aiplatform.PipelineJobSchedule.list(\n",
    "    filter='display_name=\"DISPLAY_NAME\"',\n",
    "    order_by='create_time desc'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47dc1e5",
   "metadata": {},
   "source": [
    "### Retrieve a schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ca1ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "\n",
    "pipeline_job_schedule = aiplatform.PipelineJobSchedule.get(\n",
    "    schedule_id=SCHEDULE_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd895cfb",
   "metadata": {},
   "source": [
    "### Pause a schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20aa57e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "\n",
    "pipeline_job_schedule = aiplatform.PipelineJobSchedule.get(\n",
    "    schedule_id=SCHEDULE_ID)\n",
    "\n",
    "pipeline_job_schedule.pause()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5ce1f4",
   "metadata": {},
   "source": [
    "### Update a schedule\n",
    "When you update a schedule, new runs are scheduled based on the frequency of the updated schedule. New runs are no longer created based on the old schedule and any queued runs are dropped. Pipeline runs that are already created by the old schedule aren't paused or canceled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9926ce24",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_job_schedule.update(\n",
    "    display_name='DISPLAY_NAME',\n",
    "    max_concurrent_run_count=MAX_CONCURRENT_RUN_COUNT,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d5561c",
   "metadata": {},
   "source": [
    "### Resume a schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd10cfdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_job_schedule.resume(catch_up=CATCH_UP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d365ba5b",
   "metadata": {},
   "source": [
    "***CATCH_UP***: (Optional) Indicate whether the paused schedule should backfill the skipped pipeline runs. To backfill and reschedule the skipped pipeline runs, enter the following:  \n",
    "`{ \"catch_up\":true }`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8874e08c",
   "metadata": {},
   "source": [
    "### Delete a schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1097391a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_job_schedule.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03d40fd",
   "metadata": {},
   "source": [
    "### List all pipeline jobs created by a schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e8c5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_job_schedule.list_jobs(order_by='create_time_desc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16eb562",
   "metadata": {},
   "source": [
    "## Trigger a pipeline run with Cloud Pub/Sub\n",
    "write, deploy, and trigger a pipeline using an Event-Driven Cloud Function with a Cloud Pub/Sub trigger."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c90dcd1",
   "metadata": {},
   "source": [
    "### Build and compile a simple Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aae4581",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp import compiler\n",
    "from kfp import dsl\n",
    "\n",
    "# A simple component that prints and returns a greeting string\n",
    "@dsl.component\n",
    "def hello_world(message: str) -> str:\n",
    "    greeting_str = f'Hello, {message}'\n",
    "    print(greeting_str)\n",
    "    return greeting_str\n",
    "\n",
    "# A simple pipeline that contains a single hello_world task\n",
    "@dsl.pipeline(\n",
    "    name='hello-world-scheduled-pipeline')\n",
    "def hello_world_scheduled_pipeline(greet_name: str):\n",
    "    hello_world_task = hello_world(greet_name)\n",
    "\n",
    "\n",
    "# Compile the pipeline and generate a YAML file\n",
    "compiler.Compiler().compile(pipeline_func=hello_world_scheduled_pipeline,\n",
    "                            package_path='hello_world_scheduled_pipeline.yaml')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38bbbea",
   "metadata": {},
   "source": [
    "**Upload compiled pipeline YAML to Cloud Storage bucket**  \n",
    "Click the uploaded YAML file to access the details. Copy the gsutil URI for later use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4face139",
   "metadata": {},
   "source": [
    "### Create a Cloud Function with Pub/Sub Trigger\n",
    "Use foolowing `main.py` script for in **Inline Editor** under **Source Code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451456e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "  import base64\n",
    "  import json\n",
    "  from google.cloud import aiplatform\n",
    "\n",
    "  PROJECT_ID = 'your-project-id'                     # <---CHANGE THIS\n",
    "  REGION = 'your-region'                             # <---CHANGE THIS\n",
    "  PIPELINE_ROOT = 'your-cloud-storage-pipeline-root' # <---CHANGE THIS\n",
    "\n",
    "  def subscribe(event, context):\n",
    "    \"\"\"Triggered from a message on a Cloud Pub/Sub topic.\n",
    "    Args:\n",
    "          event (dict): Event payload.\n",
    "          context (google.cloud.functions.Context): Metadata for the event.\n",
    "    \"\"\"\n",
    "    # decode the event payload string\n",
    "    payload_message = base64.b64decode(event['data']).decode('utf-8')\n",
    "    # parse payload string into JSON object\n",
    "    payload_json = json.loads(payload_message)\n",
    "    # trigger pipeline run with payload\n",
    "    trigger_pipeline_run(payload_json)\n",
    "\n",
    "  def trigger_pipeline_run(payload_json):\n",
    "    \"\"\"Triggers a pipeline run\n",
    "    Args:\n",
    "          payload_json: expected in the following format:\n",
    "            {\n",
    "              \"pipeline_spec_uri\": \"<path-to-your-compiled-pipeline>\",\n",
    "              \"parameter_values\": {\n",
    "                \"greet_name\": \"<any-greet-string>\"\n",
    "              }\n",
    "            }\n",
    "    \"\"\"\n",
    "    pipeline_spec_uri = payload_json['pipeline_spec_uri']\n",
    "    parameter_values = payload_json['parameter_values']\n",
    "\n",
    "    # Create a PipelineJob using the compiled pipeline from pipeline_spec_uri\n",
    "    aiplatform.init(\n",
    "        project=PROJECT_ID,\n",
    "        location=REGION,\n",
    "    )\n",
    "    job = aiplatform.PipelineJob(\n",
    "        display_name='hello-world-pipeline-cloud-function-invocation',\n",
    "        template_path=pipeline_spec_uri,\n",
    "        pipeline_root=PIPELINE_ROOT,\n",
    "        enable_caching=False,\n",
    "        parameter_values=parameter_values\n",
    "    )\n",
    "\n",
    "    # Submit the PipelineJob\n",
    "    job.submit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609d2310",
   "metadata": {},
   "source": [
    "In the `requirements.txt` file, replace the contents with the following package requirements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e957dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "google-api-python-client>=1.7.8,<2\n",
    "google-cloud-aiplatform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f16de03",
   "metadata": {},
   "source": [
    "## Send a notification from a pipeline\n",
    "The following example shows how to configure email notifications by defining an email notification task (`notify_email_task`) and adding it to the pipeline's exit handler (`dsl.ExitHandler`). This notification task invokes the `VertexNotificationEmailOp` operator in the email notification component when the pipeline exits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06bc11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp import dsl\n",
    "from kfp import compiler\n",
    "from google_cloud_pipeline_components.v1.vertex_notification_email import VertexNotificationEmailOp\n",
    "\n",
    "@dsl.pipeline(\n",
    "    name='PIPELINE_NAME',\n",
    "    pipeline_root=PIPELINE_ROOT_PATH,\n",
    ")\n",
    "def TASK_NAME():    # The name of the pipeline task for which you're configuring email notifications.\n",
    "    notify_email_task = VertexNotificationEmailOp(recipients=RECIPIENTS_LIST)   # A comma-separated list of up to three email addresses to send the notification email to.\n",
    "\n",
    "    with dsl.ExitHandler(notify_email_task):\n",
    "        # Add your pipeline tasks here.\n",
    "\n",
    "compiler.Compiler().compile(pipeline_func=notification_email_pipeline,\n",
    "        package_path='notification_email_pipeline.yaml')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
